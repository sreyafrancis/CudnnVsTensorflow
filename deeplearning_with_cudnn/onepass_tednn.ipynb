{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5110\n",
      "Handle Creation Status: CUDNN_STATUS_SUCCESS\n",
      "Input Tensor Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Input Grad Tensor Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Output Tensor Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Output Grad Tensor Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Filter Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Filter Grad Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Bias Grad Descriptor: CUDNN_STATUS_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ctypes\n",
    "import ctypes.util\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import skcuda.cublas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "try:\n",
    "        libcudnn=ctypes.cdll.LoadLibrary('/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10')\n",
    "except OSError:\n",
    "        print(\"OSError\");\n",
    "print(libcudnn.cudnnGetVersion())\n",
    "libcudnn.cudnnGetErrorString.restype = ctypes.c_char_p\n",
    "libcudnn.cudnnGetErrorString.argtypes = [ctypes.c_int]\n",
    "\n",
    "def cudnnCheckStatus(status):\n",
    "    print(libcudnn.cudnnGetErrorString(status))\n",
    "\n",
    "#**********Defining Handle***************\n",
    "handle=ctypes.c_void_p()\n",
    "print(\"Handle Creation Status:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreate(ctypes.byref(handle)))\n",
    "\n",
    "\n",
    "#************Image Data****************\n",
    "def normalize(image_data):\n",
    "    a = 0.1; b = 0.9; MIN = 0; MAX = 255\n",
    "    b=a + (((image_data - MIN)*(b - a))/(MAX - MIN))\n",
    "    return b\n",
    "\n",
    "img = cv2.imread('One.jpg',0)\n",
    "img=np.asarray(img)\n",
    "img = normalize(img)\n",
    "\n",
    "#************Enums*****************\n",
    "tensor_format=0\n",
    "data_type=1\n",
    "\n",
    "#***********Creating descriptors******\n",
    "\n",
    "input_desc = ctypes.c_void_p()\n",
    "print(\"Input Tensor Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(input_desc)))\n",
    "\n",
    "input_grad_desc = ctypes.c_void_p()\n",
    "print(\"Input Grad Tensor Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(input_grad_desc)))\n",
    "\n",
    "output_desc = ctypes.c_void_p()\n",
    "print(\"Output Tensor Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(output_desc)))\n",
    "\n",
    "output_grad_desc = ctypes.c_void_p()\n",
    "print(\"Output Grad Tensor Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(output_grad_desc)))\n",
    "\n",
    "\n",
    "filter_desc = ctypes.c_void_p()\n",
    "print(\"Filter Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateFilterDescriptor(ctypes.byref(filter_desc)))\n",
    "\n",
    "filter_grad_desc = ctypes.c_void_p()\n",
    "print(\"Filter Grad Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateFilterDescriptor(ctypes.byref(filter_grad_desc)))\n",
    "\n",
    "bias_grad_desc = ctypes.c_void_p()\n",
    "print(\"Bias Grad Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateFilterDescriptor(ctypes.byref(bias_grad_desc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Filter Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Convolution Desriptor: CUDNN_STATUS_SUCCESS\n",
      "Getting Output Dimensions: CUDNN_STATUS_SUCCESS\n",
      "('Output Dimensions: ', 1, 1, 27, 27)\n",
      "Setting Output Descriptor: CUDNN_STATUS_SUCCESS\n",
      "CUDNN_STATUS_SUCCESS\n",
      "Getting Workspace Size: CUDNN_STATUS_SUCCESS\n",
      "('Workspace Size = ', c_ulong(0L))\n",
      "Workspace Not allocated\n",
      "None\n",
      "Convolution forward Status: CUDNN_STATUS_SUCCESS\n",
      "Input to convolution\n",
      "[[[[ 0.76509804  0.76509804  0.76509804  0.76509804  0.76509804  0.76509804\n",
      "     0.76509804  0.76509804  0.76509804  0.76196078  0.76823529  0.77764706\n",
      "     0.77137255  0.75882353  0.77764706  0.75568627  0.76823529  0.76823529\n",
      "     0.76823529  0.76823529  0.76823529  0.76823529  0.76823529  0.76823529\n",
      "     0.76509804  0.76509804  0.76509804  0.76509804]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.89372549  0.9         0.89372549\n",
      "     0.89686275  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.88431373  0.88117647  0.9         0.9         0.9\n",
      "     0.89372549  0.89372549  0.89686275  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.88431373  0.88431373  0.9         0.87490196\n",
      "     0.87490196  0.87176471  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.87803922  0.9         0.9         0.77764706  0.4827451\n",
      "     0.41058824  0.57686275  0.86862745  0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.86862745  0.64901961  0.23490196\n",
      "     0.12509804  0.33843137  0.74627451  0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88117647  0.88745098  0.89372549  0.70235294  0.24745098\n",
      "     0.1         0.31960784  0.79333333  0.89686275  0.89686275  0.89686275\n",
      "     0.89686275  0.89686275  0.89686275  0.89686275  0.89686275  0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.89372549  0.9         0.74313725  0.30392157\n",
      "     0.13137255  0.29764706  0.74941176  0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89686275  0.89058824  0.89058824  0.70862745  0.25372549\n",
      "     0.12509804  0.29764706  0.75254902  0.89372549  0.9         0.88431373\n",
      "     0.9         0.9         0.9         0.88745098  0.9         0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.85921569  0.6427451   0.23490196\n",
      "     0.1         0.26        0.6145098   0.82470588  0.9         0.89686275\n",
      "     0.9         0.89058824  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.76509804  0.41058824\n",
      "     0.10313725  0.14392157  0.33215686  0.74        0.89058824  0.9         0.9\n",
      "     0.88117647  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89058824  0.89372549  0.9         0.87803922  0.58941176\n",
      "     0.11882353  0.12196078  0.26941176  0.69921569  0.88431373  0.9         0.9\n",
      "     0.88431373  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.88745098  0.9         0.9         0.65215686\n",
      "     0.1         0.12509804  0.28823529  0.70235294  0.89058824  0.9         0.9\n",
      "     0.89686275  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89686275  0.9         0.9         0.9         0.65529412\n",
      "     0.1         0.10941176  0.26        0.70862745  0.9         0.89058824\n",
      "     0.9         0.9         0.88745098  0.89686275  0.9         0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89058824  0.89686275  0.9         0.87176471  0.63333333\n",
      "     0.11568627  0.1345098   0.30392157  0.69607843  0.9         0.89058824\n",
      "     0.9         0.9         0.88745098  0.9         0.9         0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.89372549  0.9         0.9         0.66156863\n",
      "     0.11882353  0.1         0.26627451  0.67411765  0.9         0.89372549\n",
      "     0.89686275  0.9         0.88745098  0.9         0.89686275  0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.87176471  0.9         0.65843137\n",
      "     0.1         0.11882353  0.18156863  0.53607843  0.89058824  0.88117647\n",
      "     0.9         0.9         0.87490196  0.89372549  0.9         0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.87176471  0.9         0.9         0.9         0.6772549\n",
      "     0.18470588  0.1         0.11568627  0.36666667  0.78392157  0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.88745098  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.88431373  0.74\n",
      "     0.36666667  0.11882353  0.12196078  0.24745098  0.6427451   0.88745098\n",
      "     0.87803922  0.88117647  0.88745098  0.9         0.9         0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.88745098  0.9         0.9         0.83411765\n",
      "     0.52352941  0.11568627  0.1         0.22235294  0.5454902   0.9         0.9\n",
      "     0.9         0.89372549  0.89686275  0.87803922  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.88117647  0.9         0.9         0.87490196\n",
      "     0.66470588  0.21607843  0.12823529  0.1         0.40431373  0.83411765\n",
      "     0.87803922  0.9         0.89058824  0.9         0.9         0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.89686275  0.89686275  0.88745098  0.89058824\n",
      "     0.78705882  0.35411765  0.10941176  0.1         0.42627451  0.8654902\n",
      "     0.89372549  0.89372549  0.89372549  0.9         0.89686275  0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.84039216  0.49215686  0.1627451   0.11568627  0.41372549  0.81215686\n",
      "     0.88745098  0.9         0.9         0.89686275  0.87803922  0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.88745098  0.89686275  0.9         0.88745098\n",
      "     0.88431373  0.77137255  0.56117647  0.52666667  0.66470588  0.87490196\n",
      "     0.9         0.9         0.88431373  0.89058824  0.9         0.9         0.9\n",
      "     0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]]]]\n",
      "Convolution Output\n",
      "[[[[ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.89372549  0.9         0.89372549  0.89686275\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.88431373  0.88117647  0.9         0.9         0.9\n",
      "     0.89372549  0.89372549  0.89686275  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88431373  0.88431373  0.9         0.87490196  0.87490196\n",
      "     0.87176471  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.87803922  0.9         0.9         0.77764706  0.4827451   0.41058824\n",
      "     0.57686275  0.86862745  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.86862745  0.64901961  0.23490196  0.12509804\n",
      "     0.33843137  0.74627451  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.88117647  0.88745098  0.89372549  0.70235294  0.24745098  0.1\n",
      "     0.31960784  0.79333333  0.89686275  0.89686275  0.89686275  0.89686275\n",
      "     0.89686275  0.89686275  0.89686275  0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.74313725  0.30392157  0.13137255\n",
      "     0.29764706  0.74941176  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89686275  0.89058824  0.89058824  0.70862745  0.25372549  0.12509804\n",
      "     0.29764706  0.75254902  0.89372549  0.9         0.88431373  0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.85921569  0.6427451   0.23490196  0.1         0.26\n",
      "     0.6145098   0.82470588  0.9         0.89686275  0.9         0.89058824\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89372549  0.9         0.9         0.76509804  0.41058824  0.10313725\n",
      "     0.14392157  0.33215686  0.74        0.89058824  0.9         0.9\n",
      "     0.88117647  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89058824  0.89372549  0.9         0.87803922  0.58941176  0.11882353\n",
      "     0.12196078  0.26941176  0.69921569  0.88431373  0.9         0.9\n",
      "     0.88431373  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.65215686  0.1\n",
      "     0.12509804  0.28823529  0.70235294  0.89058824  0.9         0.9\n",
      "     0.89686275  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89686275  0.9         0.9         0.9         0.65529412  0.1\n",
      "     0.10941176  0.26        0.70862745  0.9         0.89058824  0.9         0.9\n",
      "     0.88745098  0.89686275  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89058824  0.89686275  0.9         0.87176471  0.63333333  0.11568627\n",
      "     0.1345098   0.30392157  0.69607843  0.9         0.89058824  0.9         0.9\n",
      "     0.88745098  0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.66156863  0.11882353\n",
      "     0.1         0.26627451  0.67411765  0.9         0.89372549  0.89686275\n",
      "     0.9         0.88745098  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.87176471  0.9         0.65843137  0.1\n",
      "     0.11882353  0.18156863  0.53607843  0.89058824  0.88117647  0.9         0.9\n",
      "     0.87490196  0.89372549  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.87176471  0.9         0.9         0.9         0.6772549   0.18470588\n",
      "     0.1         0.11568627  0.36666667  0.78392157  0.9         0.9         0.9\n",
      "     0.89372549  0.9         0.88745098  0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.88431373  0.74        0.36666667\n",
      "     0.11882353  0.12196078  0.24745098  0.6427451   0.88745098  0.87803922\n",
      "     0.88117647  0.88745098  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.83411765  0.52352941\n",
      "     0.11568627  0.1         0.22235294  0.5454902   0.9         0.9         0.9\n",
      "     0.89372549  0.89686275  0.87803922  0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88117647  0.9         0.9         0.87490196  0.66470588\n",
      "     0.21607843  0.12823529  0.1         0.40431373  0.83411765  0.87803922\n",
      "     0.9         0.89058824  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89686275  0.89686275  0.88745098  0.89058824  0.78705882\n",
      "     0.35411765  0.10941176  0.1         0.42627451  0.8654902   0.89372549\n",
      "     0.89372549  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.9         0.84039216\n",
      "     0.49215686  0.1627451   0.11568627  0.41372549  0.81215686  0.88745098\n",
      "     0.9         0.9         0.89686275  0.87803922  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.89686275  0.9         0.88745098  0.88431373\n",
      "     0.77137255  0.56117647  0.52666667  0.66470588  0.87490196  0.9         0.9\n",
      "     0.88431373  0.89058824  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]]]]\n"
     ]
    }
   ],
   "source": [
    "#Convolution\n",
    "\n",
    "#*******Enums********************\n",
    "convolution_mode=0\n",
    "algo=0\n",
    "preference=0\n",
    "\n",
    "#**********Dimensions************\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=28\n",
    "w_i=28\n",
    "h_k=2\n",
    "w_k=2\n",
    "k=1\n",
    "pad_h=0\n",
    "pad_w=0\n",
    "stride_h=1\n",
    "stride_w=1\n",
    "upscalex=1\n",
    "upscaley=1\n",
    "\n",
    "#********GPU arrays***********\n",
    "array_type=np.float64\n",
    "#x=np.array([[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0],[13.0,14.0,15.0,16.0],]]],dtype=array_type)\n",
    "#x=np.random.rand(1,1,28,28)\n",
    "x=img.reshape((n_i,c_i,h_i,w_i))\n",
    "X=gpuarray.to_gpu(x)\n",
    "w=np.array([[[[1.0,0.0],[0.0,0.0]]]],dtype=array_type)\n",
    "W=gpuarray.to_gpu(w)\n",
    "\n",
    "#***********Creating descriptors******\n",
    "conv_desc = ctypes.c_void_p()\n",
    "print(\"Convolution Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateConvolutionDescriptor(ctypes.byref(conv_desc)))\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "print(\"Setting Filter Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetFilter4dDescriptor(filter_desc,data_type,tensor_format,k,c_i,h_k,w_k));\n",
    "print(\"Setting Convolution Desriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetConvolution2dDescriptor(conv_desc,pad_h,pad_w,stride_h,stride_w,upscalex,upscaley,convolution_mode));\n",
    "        \n",
    "\n",
    "#*********configuring the Output*********\n",
    "print(\"Getting Output Dimensions:\"),\n",
    "temp_n_o = ctypes.c_int()\n",
    "temp_c_o = ctypes.c_int()\n",
    "temp_h_o = ctypes.c_int()\n",
    "temp_w_o = ctypes.c_int()\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolution2dForwardOutputDim(conv_desc, input_desc,filter_desc, ctypes.byref(temp_n_o),ctypes.byref(temp_c_o), ctypes.byref(temp_h_o),ctypes.byref(temp_w_o)))\n",
    "\n",
    "n_o=temp_n_o.value\n",
    "c_o=temp_c_o.value\n",
    "w_o=temp_w_o.value\n",
    "h_o=temp_h_o.value\n",
    "print(\"Output Dimensions: \",n_o,c_o,h_o,w_o)\n",
    "Y= gpuarray.empty((n_o,c_o,h_o,w_o), array_type)\n",
    "print(\"Setting Output Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_o,c_o,h_o,w_o));\n",
    "\n",
    "#*********Setting Workspace********\n",
    "algo=ctypes.c_int()\n",
    "memory_limit=ctypes.c_size_t(1024*1024)\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolutionForwardAlgorithm(handle,input_desc,filter_desc,conv_desc,output_desc,preference,memory_limit,ctypes.byref(algo)))\n",
    "\n",
    "workspace =ctypes.c_void_p()\n",
    "workspace_size=ctypes.c_size_t(0)\n",
    "print(\"Getting Workspace Size:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolutionForwardWorkspaceSize(handle,input_desc,filter_desc,conv_desc,\\\n",
    "output_desc,algo,ctypes.byref(workspace_size)))\n",
    "print(\"Workspace Size = \",workspace_size)\n",
    "if workspace_size.value!=0:\n",
    "    workspace= drv.mem_alloc(workspace_size.value)\n",
    "    print(\"workspace Allocated\")\n",
    "else:\n",
    "    print(\"Workspace Not allocated\")\n",
    "\n",
    "#*********ConvolutionForward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "W_data = ctypes.c_void_p(int(W.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "print(workspace.value)\n",
    "#workspace_data = ctypes.c_void_p(workspace)\n",
    "print(\"Convolution forward Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnConvolutionForward(handle,ctypes.byref(a),input_desc,X_data,filter_desc,\\\n",
    "                W_data,conv_desc,algo,None,0,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "print(\"Input to convolution\")\n",
    "print(X.get())\n",
    "\n",
    "print(\"Convolution Output\")\n",
    "print(Y.get())\n",
    "\n",
    "X_conv=X\n",
    "W_conv=W\n",
    "Y_conv=Y\n",
    "output_desc_conv=output_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Activation Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Output Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Activation forward Status: CUDNN_STATUS_SUCCESS\n",
      "Relu Activation Output [[[[ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.89372549  0.9         0.89372549  0.89686275\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.88431373  0.88117647  0.9         0.9         0.9\n",
      "     0.89372549  0.89372549  0.89686275  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88431373  0.88431373  0.9         0.87490196  0.87490196\n",
      "     0.87176471  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.87803922  0.9         0.9         0.77764706  0.4827451   0.41058824\n",
      "     0.57686275  0.86862745  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.86862745  0.64901961  0.23490196  0.12509804\n",
      "     0.33843137  0.74627451  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.88117647  0.88745098  0.89372549  0.70235294  0.24745098  0.1\n",
      "     0.31960784  0.79333333  0.89686275  0.89686275  0.89686275  0.89686275\n",
      "     0.89686275  0.89686275  0.89686275  0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.74313725  0.30392157  0.13137255\n",
      "     0.29764706  0.74941176  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89686275  0.89058824  0.89058824  0.70862745  0.25372549  0.12509804\n",
      "     0.29764706  0.75254902  0.89372549  0.9         0.88431373  0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.85921569  0.6427451   0.23490196  0.1         0.26\n",
      "     0.6145098   0.82470588  0.9         0.89686275  0.9         0.89058824\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89372549  0.9         0.9         0.76509804  0.41058824  0.10313725\n",
      "     0.14392157  0.33215686  0.74        0.89058824  0.9         0.9\n",
      "     0.88117647  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89058824  0.89372549  0.9         0.87803922  0.58941176  0.11882353\n",
      "     0.12196078  0.26941176  0.69921569  0.88431373  0.9         0.9\n",
      "     0.88431373  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.65215686  0.1\n",
      "     0.12509804  0.28823529  0.70235294  0.89058824  0.9         0.9\n",
      "     0.89686275  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89686275  0.9         0.9         0.9         0.65529412  0.1\n",
      "     0.10941176  0.26        0.70862745  0.9         0.89058824  0.9         0.9\n",
      "     0.88745098  0.89686275  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89058824  0.89686275  0.9         0.87176471  0.63333333  0.11568627\n",
      "     0.1345098   0.30392157  0.69607843  0.9         0.89058824  0.9         0.9\n",
      "     0.88745098  0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.66156863  0.11882353\n",
      "     0.1         0.26627451  0.67411765  0.9         0.89372549  0.89686275\n",
      "     0.9         0.88745098  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.87176471  0.9         0.65843137  0.1\n",
      "     0.11882353  0.18156863  0.53607843  0.89058824  0.88117647  0.9         0.9\n",
      "     0.87490196  0.89372549  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.87176471  0.9         0.9         0.9         0.6772549   0.18470588\n",
      "     0.1         0.11568627  0.36666667  0.78392157  0.9         0.9         0.9\n",
      "     0.89372549  0.9         0.88745098  0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.88431373  0.74        0.36666667\n",
      "     0.11882353  0.12196078  0.24745098  0.6427451   0.88745098  0.87803922\n",
      "     0.88117647  0.88745098  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.83411765  0.52352941\n",
      "     0.11568627  0.1         0.22235294  0.5454902   0.9         0.9         0.9\n",
      "     0.89372549  0.89686275  0.87803922  0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88117647  0.9         0.9         0.87490196  0.66470588\n",
      "     0.21607843  0.12823529  0.1         0.40431373  0.83411765  0.87803922\n",
      "     0.9         0.89058824  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89686275  0.89686275  0.88745098  0.89058824  0.78705882\n",
      "     0.35411765  0.10941176  0.1         0.42627451  0.8654902   0.89372549\n",
      "     0.89372549  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.9         0.84039216\n",
      "     0.49215686  0.1627451   0.11568627  0.41372549  0.81215686  0.88745098\n",
      "     0.9         0.9         0.89686275  0.87803922  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.89686275  0.9         0.88745098  0.88431373\n",
      "     0.77137255  0.56117647  0.52666667  0.66470588  0.87490196  0.9         0.9\n",
      "     0.88431373  0.89058824  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]]]]\n"
     ]
    }
   ],
   "source": [
    "#Relu Activation\n",
    "\n",
    "X_relu=Y_conv\n",
    "X=X_relu\n",
    "#input_desc=output_desc_conv\n",
    "\n",
    "#**********Defining Enumerated Types*************\n",
    "cudnnDataType={'CUDNN_DATA_FLOAT': 0}\n",
    "cudnnTensorFormat = {'CUDNN_TENSOR_NCHW': 0}\n",
    "#cudnnPoolingMode       ={'CUDNN_POOLING_MAX':0}\n",
    "cudnnNanPropagation     ={'CUDNN_NOT_PROPAGATE_NAN':0}\n",
    "cudnnActivationMode     ={'CUDNN_ACTIVATION_RELU':1}\n",
    "\n",
    "relu_mode = cudnnActivationMode['CUDNN_ACTIVATION_RELU']\n",
    "reluNanOpt = cudnnNanPropagation ['CUDNN_NOT_PROPAGATE_NAN']\n",
    "\n",
    "\n",
    "#**********Dimensions************\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=4\n",
    "w_i=4\n",
    "\"\"\"\n",
    "n_i=n_o\n",
    "c_i=c_o\n",
    "h_i=h_o\n",
    "w_i=w_o\n",
    "\n",
    "\n",
    "#********GPU arrays***********\n",
    "\"\"\"\n",
    "X = gpuarray.to_gpu(np.random.rand(n_i,c_i,h_i,w_i)\n",
    ".astype(np.float32))\n",
    "W = gpuarray.to_gpu(np.random.rand(k,h_k,w_k).astype(np.float32))\n",
    "#x=np.array([[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0],[13.0,14.0,15.0,16.0]]]],dtype='float32')\n",
    "x=np.array([[[[1.0,-2.0,3.0,-4.0],[5.0,-6.0,-7.0,-8.0],[-9.0,-10.0,-11.0,-12.0],[-13.0,14.0,-15.0,-16.0]]]],dtype='float32')\n",
    "X=gpuarray.to_gpu(x)\n",
    "\"\"\"\n",
    "\n",
    "#*******Creating descriptors********\n",
    "\n",
    "activation_desc=ctypes.c_void_p()\n",
    "print(\"Activation Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateActivationDescriptor(ctypes.byref(activation_desc)))\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "reluCeiling=ctypes.c_double(1.0)\n",
    "maxpoolingNanOpt =cudnnNanPropagation['CUDNN_NOT_PROPAGATE_NAN']\n",
    "print(\"Setting Activation Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetActivationDescriptor(activation_desc,relu_mode,reluNanOpt,ctypes.byref(reluCeiling)))\n",
    "\n",
    "\n",
    "#*********configuring the Output*****************************************************************\n",
    "Y = gpuarray.empty((n_i,c_i,h_i,w_i), np.float64)\n",
    "\n",
    "\n",
    "print(\"Setting Output Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "\n",
    "\n",
    "#*********PoolingForward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "print(\"Activation forward Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnActivationForward(handle,activation_desc,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "\n",
    "#*********Display Output*************\n",
    "\n",
    "\n",
    "print(\"Relu Activation Output\"),\n",
    "print(Y.get())\n",
    "Y_relu=Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Pooling Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Getting Output Dimensions: CUDNN_STATUS_SUCCESS\n",
      "('Output Dimensions: ', 1, 1, 24, 24) Setting Output Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Pooling forward Status: CUDNN_STATUS_SUCCESS\n",
      "Max Pooling Output\n",
      "[[[[ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.77764706  0.86862745  0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.74313725  0.79333333  0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.74313725  0.79333333  0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.76509804  0.75254902  0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.87803922  0.75254902  0.89372549\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.65215686  0.82470588\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.65529412  0.74        0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.65529412  0.70862745\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.66156863  0.70862745\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.66156863  0.70862745\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.6772549   0.69607843\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.74        0.67411765\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.83411765  0.53607843\n",
      "     0.89058824  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.87490196  0.66470588\n",
      "     0.78392157  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.89058824  0.78705882\n",
      "     0.6427451   0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.84039216\n",
      "     0.5454902   0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.88431373\n",
      "     0.77137255  0.87490196  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]]]]\n"
     ]
    }
   ],
   "source": [
    "#Maxpool\n",
    "X_pool=Y_relu\n",
    "X=X_pool\n",
    "#**********Defining Enumerated Types*************\n",
    "\n",
    "cudnnDataType={'CUDNN_DATA_FLOAT': 0}\n",
    "cudnnTensorFormat = {'CUDNN_TENSOR_NCHW': 0}\n",
    "cudnnPoolingMode        ={'CUDNN_POOLING_MAX':0}\n",
    "cudnnNanPropagation     ={'CUDNN_NOT_PROPAGATE_NAN':0}\n",
    "\n",
    "pooling_mode=0\n",
    "maxpoolingNanOpt =0\n",
    "\n",
    "#**********Dimensions************\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=4\n",
    "w_i=4\n",
    "\"\"\"\n",
    "n_i=n_o\n",
    "c_i=c_o\n",
    "h_i=h_o\n",
    "w_i=w_o\n",
    "\n",
    "pad_h=0\n",
    "pad_w=0\n",
    "stride_h=1\n",
    "stride_w=1\n",
    "win_h=4\n",
    "win_w=4\n",
    "\n",
    "#********GPU arrays***********\n",
    "#x=np.array([[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0],[13.0,14.0,15.0,16.0]]]],dtype='float32')\n",
    "#X=gpuarray.to_gpu(x)\n",
    "\n",
    "\n",
    "\n",
    "#*******Creating descriptors********\n",
    "\n",
    "pooling_desc=ctypes.c_void_p()\n",
    "print(\"Pooling Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreatePoolingDescriptor(ctypes.byref(pooling_desc)))\n",
    "\n",
    "#********Set descriptors*******\n",
    "\n",
    "print(\"Setting Input Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "print(\"Setting Pooling Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetPooling2dDescriptor(pooling_desc,pooling_mode,maxpoolingNanOpt,\n",
    "win_h,\n",
    "win_w,\n",
    "pad_h,\n",
    "pad_w,\n",
    "stride_h,\n",
    "stride_w))\n",
    "#*********configuring the Output*****************************************************************\n",
    "\n",
    "print(\"Getting Output Dimensions:\"),\n",
    "temp_n_o = ctypes.c_int()\n",
    "temp_c_o = ctypes.c_int()\n",
    "temp_h_o = ctypes.c_int()\n",
    "temp_w_o = ctypes.c_int()\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnGetPooling2dForwardOutputDim(pooling_desc,input_desc,ctypes.byref(temp_n_o),ctypes.byref(temp_c_o), ctypes.byref(temp_h_o),ctypes.byref(temp_w_o)))\n",
    "n_o=temp_n_o.value\n",
    "c_o=temp_c_o.value\n",
    "w_o=temp_w_o.value\n",
    "h_o=temp_h_o.value\n",
    "\n",
    "\n",
    "print(\"Output Dimensions: \",n_o,c_o,h_o,w_o),\n",
    "Y = gpuarray.empty((n_o,c_o,h_o,w_o), np.float64)\n",
    "\n",
    "\n",
    "print(\"Setting Output Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_o,c_o,h_o,w_o));\n",
    "\n",
    "\n",
    "#*********PoolingForward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_float(0.0)\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "print(\"Pooling forward Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnPoolingForward(handle,pooling_desc,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "\n",
    "#*********Display Output*************\n",
    "\n",
    "print(\"Max Pooling Output\")\n",
    "print(Y.get())\n",
    "\n",
    "\n",
    "Y_pool=Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batchnorm TensorDescriptor:  CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output  Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting BatchNorm Descriptor:  CUDNN_STATUS_SUCCESS\n",
      "BatchNorm Status: CUDNN_STATUS_SUCCESS\n",
      "Batchnorm Output:\n",
      "[[[[ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.47613948 -0.47122267\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.75267973 -3.02921998\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -0.26381748 -3.02921998\n",
      "     0.08185783  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.24154198\n",
      "    -1.43911354  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.30576023  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -4.68846148\n",
      "    -4.2736511   0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -3.30576023\n",
      "    -4.75759654  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -1.23170835\n",
      "    -7.79953929  0.01272277  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -0.33295254\n",
      "    -4.96500173 -2.33786935  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.01272277\n",
      "    -2.26873429 -5.44894716  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -1.09343823 -7.5921341   0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -0.12554735 -2.6144096  -0.33295254  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]]]]\n"
     ]
    }
   ],
   "source": [
    "# Batchnorm\n",
    "X_norm=Y_pool\n",
    "X=X_norm\n",
    "\n",
    "#**********Defining Enumerated Types*************\n",
    "\"\"\"\n",
    "cudnnDataType={'CUDNN_DATA_FLOAT': 0}\n",
    "cudnnTensorFormat = {'CUDNN_TENSOR_NCHW': 0}\n",
    "cudnnSoftmaxAlgorithm ={'CUDNN_SOFTMAX_FAST':0}\n",
    "\n",
    "BatchNorm Modes\n",
    "CUDNN_BATCHNORM_PER_ACTIVATION : to be used after non-convolution layers. bnBias and bnScale dimensions are 1xCxHxW\n",
    "CUDNN_BATCHNORM_SPATIAL : to be used after convolutional layers. bnBias and bnScale dimensions are 1xCx1x1 \n",
    "\n",
    "cudnnBatchNormMode={'CUDNN_BATCHNORM_PER_ACTIVATION':0,'CUDNN_BATCHNORM_SPATIAL': 1}\n",
    "\"\"\"\n",
    "\n",
    "data_type =1\n",
    "batchnorm_mode=1\n",
    "\n",
    "#**********Dimensions************\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=1\n",
    "w_i=4\n",
    "\"\"\"\n",
    "n_i=n_o\n",
    "c_i=c_o\n",
    "h_i=h_o\n",
    "w_i=w_o\n",
    "\n",
    "#********GPU arrays***********\n",
    "\"\"\"\n",
    "X = gpuarray.to_gpu(np.random.rand(n_i,c_i,h_i,w_i)\n",
    ".astype(np.float32))\n",
    "W = gpuarray.to_gpu(np.random.rand(k,h_k,w_k).astype(np.float32))\n",
    "x=np.array([[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0],[13.0,14.0,15.0,16.0]]]],dtype='float32')\n",
    "\"\"\"\n",
    "#x=np.array([[[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]]]])\n",
    "#x=np.array([[[[1.0,2.3,4.1,0.6]]]],dtype='float64')\n",
    "#X=gpuarray.to_gpu(x)\n",
    "Y = gpuarray.empty((n_i,c_i,h_i,w_i), np.float64)\n",
    "\n",
    "#*******Creating descriptors********\n",
    "\n",
    "batchnorm_desc=ctypes.c_void_p()\n",
    "print(\"Batchnorm TensorDescriptor: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(batchnorm_desc)))\n",
    "\n",
    "#********Set descriptors*******\n",
    "\n",
    "print(\"Setting Input Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "print(\"Setting Output  Descriptor:\"),        \n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "\n",
    "if batchnorm_mode==0:\n",
    "\tn_bn=1\n",
    "\tc_bn=c_i\n",
    "\th_bn=h_i\n",
    "\tw_bn=w_i\n",
    "else:\n",
    "\tn_bn=1\n",
    "\tc_bn=c_i\n",
    "\th_bn=1\n",
    "\tw_bn=1\n",
    "\n",
    "print(\"Setting BatchNorm Descriptor: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(batchnorm_desc,tensor_format,data_type,n_bn,c_bn,h_bn,w_bn))\n",
    "\n",
    "\n",
    "#*********BatchNorm Forward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "scale_array=np.ones((n_bn,c_bn,h_bn,w_bn),dtype='float64')\n",
    "scale=gpuarray.to_gpu(scale_array) #gamma\n",
    "scale_p=ctypes.c_void_p(int(scale.gpudata)) \n",
    "\n",
    "bias_array=np.zeros((n_bn,c_bn,h_bn,w_bn),dtype='float64')\n",
    "bias=gpuarray.to_gpu(bias_array) #beta\n",
    "bias_p=ctypes.c_void_p(int(bias.gpudata))\n",
    "\n",
    "exp_avg_factor=ctypes.c_double(0.1) #momentum\n",
    "\n",
    "#running_mean=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float32)\n",
    "#running_var=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float32)\n",
    "\n",
    "running_mean_array=np.zeros((n_bn,c_bn,h_bn,w_bn),dtype='float64')\n",
    "running_mean=gpuarray.to_gpu(running_mean_array)\n",
    "running_mean_p=ctypes.c_void_p(int(running_mean.gpudata))\n",
    "\n",
    "running_var_array=np.ones((n_bn,c_bn,h_bn,w_bn),dtype='float64')\n",
    "running_var=gpuarray.to_gpu(running_var_array)\n",
    "running_var_p=ctypes.c_void_p(int(running_var.gpudata))\n",
    "\n",
    "#e=ctypes.c_float(libcudnn.CUDNN_BN_MIN_EPSILON)\n",
    "epsilon=ctypes.c_double(0.0001)\n",
    "\n",
    "result_save_mean=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float64)\n",
    "result_save_mean_p=ctypes.c_void_p(int(result_save_mean.gpudata))\n",
    "\n",
    "result_save_var=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float64)\n",
    "result_save_var_p=ctypes.c_void_p(int(result_save_var.gpudata))\n",
    "\n",
    "print(\"BatchNorm Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnBatchNormalizationForwardTraining(handle,batchnorm_mode,ctypes.byref(a),\\\n",
    "                 ctypes.byref(b),input_desc,X_data,output_desc,Y_data,batchnorm_desc,scale_p,bias_p,\\\n",
    "                 exp_avg_factor,running_mean_p,running_var_p,epsilon,result_save_mean_p,result_save_var_p))\n",
    "\n",
    "print(\"Batchnorm Output:\")\n",
    "print(Y.get())\n",
    "Y_norm=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_STATUS_SUCCESS\n",
      "CUDNN_STATUS_SUCCESS\n",
      "CUDNN_STATUS_SUCCESS\n",
      "CUDNN_STATUS_SUCCESS\n",
      "Adding Bias Tensor Status:\n",
      "CUDNN_STATUS_SUCCESS\n",
      "Input vector\n",
      "----------------------------------\n",
      "[[[[ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.47613948 -0.47122267\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.75267973 -3.02921998\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -0.26381748 -3.02921998\n",
      "     0.08185783  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.24154198\n",
      "    -1.43911354  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.30576023  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -4.68846148\n",
      "    -4.2736511   0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -3.30576023\n",
      "    -4.75759654  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -1.23170835\n",
      "    -7.79953929  0.01272277  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -0.33295254\n",
      "    -4.96500173 -2.33786935  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.01272277\n",
      "    -2.26873429 -5.44894716  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -1.09343823 -7.5921341   0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -0.12554735 -2.6144096  -0.33295254  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]]]]\n",
      "Weight Vector\n",
      "----------------------------------\n",
      "[[ 0.04714352 -0.11909757  0.1432707  ..., -0.06365235  0.00156964\n",
      "  -0.2242685 ]\n",
      " [ 0.11500357  0.0991946   0.09533241 ...,  0.02890919  0.13211582\n",
      "  -0.15469056]\n",
      " [-0.02026463 -0.06559693  0.01934214 ..., -0.18170272 -0.01831085\n",
      "   0.10589692]\n",
      " ..., \n",
      " [ 0.08577666 -0.16943387  0.04568142 ...,  0.04186456 -0.14352252\n",
      "   0.14938725]\n",
      " [-0.07106881 -0.26728005 -0.14209601 ...,  0.04735934 -0.0468891\n",
      "   0.13837672]\n",
      " [-0.09189748  0.07331661 -0.05722181 ..., -0.105043    0.06332859\n",
      "   0.0190156 ]]\n",
      "Inner product\n",
      "----------------------------------\n",
      "[[ 2.77449028 -3.17381567 -4.20744104 -1.67281785  1.5605257  -1.00702553\n",
      "   4.26789059  2.32449637  3.1051632   2.40791116]]\n"
     ]
    }
   ],
   "source": [
    "#Innerproduct\n",
    "A=Y_norm\n",
    "#skcuda.cublas.cublasCheckStatus(status)\n",
    "\"\"\"\n",
    "m=1\n",
    "k=3\n",
    "n=4\n",
    "\"\"\"\n",
    "m=1\n",
    "k=n_o*c_o*h_o*w_o\n",
    "#k=100\n",
    "n=10\n",
    "#x = np.float64(np.random.rand(m,k))\n",
    "w = np.float64(np.random.normal(0,0.1,k*n))\n",
    "w=w.reshape(k,n)\n",
    "bias=np.zeros((m,n),dtype=\"float64\")\n",
    "\n",
    "\n",
    "#A = gpuarray.to_gpu(x)\n",
    "B = gpuarray.to_gpu(w)\n",
    "C =gpuarray.empty((m,n), np.float64)\n",
    "BIAS =gpuarray.to_gpu(bias)\n",
    "h =skcuda.cublas.cublasCreate()\n",
    "\n",
    "#transa=_CUBLAS_OP[transa]\n",
    "\n",
    "lda=m\n",
    "ldb=k\n",
    "ldc=m\n",
    "#alf = 1.0;\n",
    "#bet = 0.0;\n",
    "#const float *alpha = &alf;\n",
    "#const float *beta = &bet;\n",
    "alpha=1.0\n",
    "beta=0.0\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(input_desc)))\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(output_desc)))\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "\n",
    "\n",
    "#d=skcuda.cublas.cublasSgemm(h, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc)\n",
    "status=skcuda.cublas.cublasDgemm(h, 0, 0, m, n, k, alpha,A.gpudata, lda,B.gpudata, ldb, beta,C.gpudata, ldc)\n",
    "#skcuda.cublas.cublasCheckStatus(status)\n",
    "\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(1.0)\n",
    "X_data = ctypes.c_void_p(int(BIAS.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(C.gpudata))\n",
    "\n",
    "\n",
    "print(\"Adding Bias Tensor Status:\")\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnAddTensor(handle,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "print 'Input vector'\n",
    "print '----------------------------------'\n",
    "print A.get()\n",
    "print 'Weight Vector'\n",
    "print '----------------------------------'\n",
    "print w\n",
    "print 'Inner product'\n",
    "print '----------------------------------'\n",
    "print C.get()\n",
    "\n",
    "n_o=1\n",
    "c_o=1\n",
    "h_o=m\n",
    "w_o=n\n",
    "\n",
    "\n",
    "Y_ip=C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Input Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output  Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Softmax Activation Status: CUDNN_STATUS_SUCCESS\n",
      "Softmax Output\n",
      "[[[[  1.17510182e-01   3.06731977e-04   1.09109171e-04   1.37604971e-03\n",
      "      3.49025680e-02   2.67783648e-03   5.23179858e-01   7.49282571e-02\n",
      "      1.63562947e-01   8.14464603e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "X=Y_ip\n",
    "\n",
    "\n",
    "#*******Enums************\n",
    "#softmax_mode = cudnnSoftmaxMode['CUDNN_SOFTMAX_MODE_INSTANCE']\n",
    "#softmax_algo = cudnnSoftmaxAlgorithm ['CUDNN_SOFTMAX_FAST']\n",
    "softmax_mode=0\n",
    "softmax_algo=0\n",
    "#**********Dimensions************\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=1\n",
    "w_i=4\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=1\n",
    "w_i=n_o*c_o*h_o*w_o\n",
    "\n",
    "#********GPU arrays***********\n",
    "#x=np.array([[[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]]]])\n",
    "#x=np.array([[[[1.0,2.3,4.1,0.6]]]],dtype='float32')\n",
    "#X=gpuarray.to_gpu(x)\n",
    "\n",
    "Y = gpuarray.empty((n_i,c_i,h_i,w_i), np.float64)\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "print(\"Setting Output  Descriptor:\"),        \n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "\n",
    "#*********Softmax Forward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "print(\"Softmax Activation Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnSoftmaxForward(handle,softmax_algo,softmax_mode,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "print(\"Softmax Output\")\n",
    "print(Y.get())\n",
    "\n",
    "Y_softmax=Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Input Gradient Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output Gradient Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Softmax Backward Status: CUDNN_STATUS_SUCCESS\n",
      "Softmax Output\n",
      "[[[[  1.17510182e-01   3.06731977e-04   1.09109171e-04   1.37604971e-03\n",
      "      3.49025680e-02   2.67783648e-03   5.23179858e-01   7.49282571e-02\n",
      "      1.63562947e-01   8.14464603e-02]]]]\n",
      "Softmax Input Gradient\n",
      "[[[[ 1.   1.   1.   1.1  1.1  1.   1.   1.   1.   1. ]]]]\n",
      "Softmax Output Gradient\n",
      "[[[[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]]]]\n"
     ]
    }
   ],
   "source": [
    "#softmax backward\n",
    "\n",
    "n_o=1\n",
    "c_o=1\n",
    "h_o=1\n",
    "w_o=10\n",
    "\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=1\n",
    "w_i=10\n",
    "\n",
    "Y= Y_softmax\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "#print(Y.get())\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(1.0)\n",
    "input_grad_array=np.array([[[[1.0,1.0,1.0,1.1,1.1,1.0,1.0,1.0,1.0,1.0]]]],dtype='float64')\n",
    "output_grad_array=np.zeros((n_o,c_o,h_o,w_o),dtype='float64')\n",
    "\n",
    "input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "#print(input_grad.get())\n",
    "\n",
    "output_grad=gpuarray.empty((n_o,c_o,h_o,w_o),np.float64)\n",
    "#output_grad=gpuarray.to_gpu(output_grad_array)\n",
    "#print(output_grad.get())\n",
    "\n",
    "\n",
    "input_grad_data=ctypes.c_void_p(int(input_grad.gpudata))\n",
    "output_grad_data=ctypes.c_void_p(int(output_grad.gpudata))\n",
    "\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Gradient Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "print(\"Setting Output Gradient Descriptor:\"),        \n",
    "#cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type,n_o,c_o,h_o,w_o))\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "#************Softmax Backward***********************\n",
    "print(\"Softmax Backward Status:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSoftmaxBackward(handle, softmax_algo, softmax_mode, ctypes.byref(a),input_desc,\\\n",
    "                 Y_data, input_grad_desc, input_grad_data, ctypes.byref(b),output_grad_desc, output_grad_data))\n",
    "\n",
    "print(\"Softmax Output\")\n",
    "print(Y.get())\n",
    "print(\"Softmax Input Gradient\")\n",
    "print(input_grad.get())\n",
    "print(\"Softmax Output Gradient\")\n",
    "print(output_grad.get())\n",
    "dY_softmax=output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 10)\n",
      "(1, 1, 24, 24)\n"
     ]
    }
   ],
   "source": [
    "print B.shape\n",
    "print A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check\n",
      "output gradient (dx)\n",
      "[[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "Weights gradient (dw)\n",
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "# ip backward\n",
    "input_grad=dY_softmax\n",
    "\n",
    "#inner Product Backward\n",
    "N,C,H,W = A.shape\n",
    "#m=N*C*H*W #input layer\n",
    "m=24*24\n",
    "k=10 #output layer\n",
    "n=1\n",
    "\"\"\"\n",
    "m=3\n",
    "k=2 \n",
    "n=1 \n",
    "\"\"\"\n",
    "print(\"Check\")\n",
    "\n",
    "input_grad=input_grad.reshape(-1,1)\n",
    "#input_grad_array=np.array([[1],[2]],dtype='float64')\n",
    "#input_grad_array=np.reshape(input_grad_array,(m,k))\n",
    "#input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "output_grad=gpuarray.empty((m,n),np.float64)\n",
    "alpha=1.0\n",
    "beta=0.0\n",
    "lda=m\n",
    "ldb=k\n",
    "ldc=m\n",
    "#ldc=n\n",
    "status=skcuda.cublas.cublasDgemm(h, 0, 0, m, n, k, alpha,B.gpudata, lda,input_grad.gpudata,ldb, beta,\\\n",
    "    output_grad.gpudata, ldc)\n",
    "#B=B.reshape((1,-1))\n",
    "#input_grad=input_grad.reshape((-1,1))\n",
    "\n",
    "m=10\n",
    "k=1\n",
    "n=N*C*H*W\n",
    "\"\"\"\n",
    "m=2 \n",
    "k=1 \n",
    "n=3 \n",
    "\"\"\"\n",
    "lda=m\n",
    "ldb=k\n",
    "ldc=m\n",
    "X=A.reshape(1,-1)\n",
    "weight_grad=gpuarray.empty((n,m),np.float64)\n",
    "status=skcuda.cublas.cublasDgemm(h, 0, 0, m, n, k, alpha,input_grad.gpudata, lda,X.gpudata,ldb, beta,\\\n",
    "    weight_grad.gpudata, ldc)\n",
    "\"\"\"\n",
    "print(\"W\")\n",
    "print(A.get())\n",
    "print(\"X\")\n",
    "print(B.get())\n",
    "\n",
    "print(\"IP output\")\n",
    "print(C.get())\n",
    "print(\"input gradient\")\n",
    "print(input_grad.get())\n",
    "\"\"\"\n",
    "print(\"output gradient (dx)\")\n",
    "print(output_grad.get())\n",
    "\n",
    "print(\"Weights gradient (dw)\")\n",
    "print(weight_grad.get())\n",
    "\n",
    "dY_ip=output_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  1.17510182e-01   3.06731977e-04   1.09109171e-04   1.37604971e-03\n",
      "      3.49025680e-02   2.67783648e-03   5.23179858e-01   7.49282571e-02\n",
      "      1.63562947e-01   8.14464603e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "#print input_grad.get()\n",
    "print Y.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Output Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Grad Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output Grad Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Batch Norm Backward status: CUDNN_STATUS_SUCCESS\n",
      "Batchnorm Output Gradient\n",
      "[[[[ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]]]]\n"
     ]
    }
   ],
   "source": [
    "#Batchnorm Backward\n",
    "n_i,c_i,h_i,w_i=X_norm.shape\n",
    "n_o,c_o,h_o,w_o=Y_norm.shape\n",
    "\n",
    "input_grad=dY_ip.reshape((n_o,c_o,h_o,w_o))\n",
    "\n",
    "Y=Y_norm\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "print(\"Setting Output Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_o,c_o,h_o,w_o))\n",
    "print(\"Setting Input Grad Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_grad_desc,tensor_format,data_type,n_o,c_o,h_o,w_o))\n",
    "print(\"Setting Output Grad Descriptor:\"),        \n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "\n",
    "\n",
    "batchnorm_grad_desc = batchnorm_desc\n",
    "\n",
    "\"\"\"\n",
    "input_grad_array=np.array([[[[1,1,1,1]]]],dtype='float64')\n",
    "input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "\"\"\"\n",
    "input_grad_data=ctypes.c_void_p(int(input_grad.gpudata))\n",
    "\n",
    "output_grad=gpuarray.empty((n_i,c_i,h_i,w_i),np.float64)\n",
    "output_grad_data=ctypes.c_void_p(int(output_grad.gpudata))\n",
    "\n",
    "scale_grad=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float64)\n",
    "scale_grad_p=ctypes.c_void_p(int(scale_grad.gpudata))\n",
    "bias_grad=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float64)\n",
    "bias_grad_p=ctypes.c_void_p(int(bias_grad.gpudata))\n",
    "\n",
    "a_data=ctypes.c_double(1.0)\n",
    "b_data=ctypes.c_double(0.0)\n",
    "a_param=ctypes.c_double(1.0)\n",
    "b_param=ctypes.c_double(0.0)\n",
    "\n",
    "print(\"Batch Norm Backward status:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnBatchNormalizationBackward(handle,batchnorm_mode,\\\n",
    "        ctypes.byref(a_data),ctypes.byref(b_data),ctypes.byref(a_param),ctypes.byref(b_param),\\\n",
    "            output_desc,Y_data,input_grad_desc,input_grad_data,output_grad_desc,output_grad_data,\\\n",
    "            batchnorm_grad_desc,scale_p,scale_grad_p,bias_grad_p,epsilon,result_save_mean_p,result_save_var_p)) \n",
    "\n",
    "#print(\"Batchnorm Output\")\n",
    "#print(Y.get())\n",
    "#print(\"Batchnorm Input Gradient\")\n",
    "#print(input_grad.get())\n",
    "print(\"Batchnorm Output Gradient\")\n",
    "print(output_grad.get())\n",
    "\n",
    "dY_norm=output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 24, 24)\n",
      "(1, 576)\n",
      "(1, 1, 24, 24)\n"
     ]
    }
   ],
   "source": [
    "print output_grad.shape\n",
    "print X.shape\n",
    "print Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Output Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Input Gradient Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output Gradient Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Pooling backward Status: CUDNN_STATUS_SUCCESS\n",
      "Output Gradient:\n",
      "[[[[ nan  nan  nan  nan  nan  nan  nan  nan  nan   0.  nan   0.   0.  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n",
      "   [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  nan  nan\n",
      "     nan   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "   [  0.   0.   0.   0.   0.   0.  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "      0.   0.  nan   0.   0.   0.  nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan   0.   0.   0.  nan  nan  nan  nan  nan\n",
      "     nan  nan   0.  nan  nan  nan   0.   0.   0.  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan   0.   0.   0.]\n",
      "   [  0.   0.   0.  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.\n",
      "      0.  nan  nan  nan  nan  nan  nan  nan   0.   0.]\n",
      "   [  0.   0.   0.   0.   0.  nan   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "     nan   0.   0.   0.  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan   0.  nan  nan   0.   0.   0.   0.  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan   0.   0.   0.  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan   0.   0.   0.   0.   0.   0.   0.  nan  nan  nan\n",
      "      0.  nan  nan  nan   0.  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.   0.   0.\n",
      "      0.  nan  nan   0.  nan   0.  nan  nan  nan  nan]\n",
      "   [  0.   0.   0.  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan\n",
      "      0.   0.   0.   0.  nan   0.  nan  nan   0.  nan]\n",
      "   [ nan   0.  nan   0.   0.   0.  nan  nan  nan  nan  nan  nan  nan   0.\n",
      "      0.  nan  nan   0.   0.   0.   0.   0.   0.  nan]\n",
      "   [ nan   0.  nan  nan   0.  nan   0.   0.   0.  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan   0.  nan  nan  nan   0.   0.   0.]\n",
      "   [  0.   0.  nan  nan   0.   0.  nan   0.  nan   0.   0.   0.  nan  nan\n",
      "     nan  nan  nan  nan  nan   0.  nan  nan  nan  nan]\n",
      "   [  0.   0.   0.  nan  nan   0.  nan  nan   0.   0.  nan  nan   0.   0.\n",
      "      0.  nan  nan  nan  nan  nan  nan  nan   0.   0.]\n",
      "   [ nan   0.   0.   0.   0.   0.  nan  nan   0.  nan  nan   0.  nan  nan\n",
      "     nan   0.   0.   0.  nan  nan  nan  nan  nan  nan]\n",
      "   [ nan  nan   0.  nan  nan  nan   0.   0.   0.  nan  nan   0.   0.  nan\n",
      "      0.  nan   0.  nan   0.   0.   0.  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan  nan  nan   0.  nan   0.   0.   0.   0.  nan  nan\n",
      "      0.  nan  nan   0.   0.  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan  nan   0.   0.\n",
      "      0.   0.  nan  nan  nan  nan   0.  nan   0.  nan]\n",
      "   [  0.   0.   0.  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.\n",
      "     nan   0.   0.   0.   0.  nan   0.   0.   0.   0.]\n",
      "   [ nan  nan  nan   0.   0.   0.  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "      0.  nan  nan  nan   0.   0.   0.   0.  nan  nan]\n",
      "   [ nan  nan   0.   0.   0.  nan   0.   0.   0.  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan   0.  nan  nan  nan  nan   0.   0.]\n",
      "   [  0.   0.   0.   0.  nan   0.  nan  nan  nan   0.   0.   0.  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan   0.   0.   0.  nan]\n",
      "   [ nan   0.   0.   0.   0.   0.   0.   0.   0.  nan   0.  nan   0.   0.\n",
      "      0.  nan  nan  nan  nan  nan  nan  nan  nan   0.]]]]\n"
     ]
    }
   ],
   "source": [
    "#Pooling Backward\n",
    "\n",
    "input_grad=dY_norm\n",
    "input_grad_data=ctypes.c_void_p(int(input_grad.gpudata))\n",
    "output_grad = gpuarray.empty((n_i,c_i,h_i,w_i), np.float64)\n",
    "output_grad_data=ctypes.c_void_p(int(output_grad.gpudata))  \n",
    "\n",
    "n_i,c_i,h_i,w_i=X_pool.shape\n",
    "n_o,c_o,h_o,w_o=Y_pool.shape\n",
    "X=X_pool\n",
    "Y=Y_pool\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "#input_grad_array=np.array([[[[1.0,1.0,1.0],[1.0,1.0,1.0],[1.0,1.0,1.0]]]],dtype='float64')\n",
    "#input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "\n",
    "                   \n",
    "#********Set descriptors*******\n",
    "print(\"Setting Output Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type, n_o, c_o, h_o, w_o))\n",
    "\n",
    "print(\"Setting Input Descriptor:\"),        \n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "print(\"Setting Input Gradient Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_grad_desc,tensor_format,data_type, n_o, c_o, h_o, w_o))\n",
    "\n",
    "print(\"Setting Output Gradient Descriptor:\"),        \n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "print(\"Pooling backward Status:\"),\n",
    "\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "#cudnnCheckStatus(libcudnn.cudnnPoolingForward(handle,pooling_desc,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "cudnnCheckStatus(libcudnn.cudnnPoolingBackward(handle,pooling_desc,ctypes.byref(a),\\\n",
    "                output_desc,Y_data,input_grad_desc,input_grad_data,\\\n",
    "                input_desc,X_data,ctypes.byref(b),output_grad_desc,output_grad_data))\n",
    "print (\"Output Gradient:\")\n",
    "print (output_grad.get())\n",
    "\n",
    "dY_pool=output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Input Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting output Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Gradient Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output Gradient Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Activation Backward Status: CUDNN_STATUS_SUCCESS\n",
      "Activation Output Gradient\n",
      "[[[[ nan  nan  nan  nan  nan  nan  nan  nan  nan   0.  nan   0.   0.  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.]\n",
      "   [  0.   0.   0.   0.   0.   0.   0.   0.   0.  nan  nan  nan   0.   0.\n",
      "      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.   0.  nan   0.   0.   0.\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan   0.   0.   0.\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.   0.   0.\n",
      "      0.  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.   0.   0.   0.   0.   0.   0.\n",
      "     nan   0.   0.   0.   0.   0.   0.   0.   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan   0.   0.   0.\n",
      "      0.  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.   0.   0.   0.   0.   0.   0.\n",
      "     nan  nan  nan   0.  nan  nan  nan   0.  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.   0.   0.\n",
      "      0.  nan  nan   0.  nan   0.  nan  nan  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan   0.   0.   0.\n",
      "      0.  nan   0.  nan  nan   0.  nan  nan   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.   0.  nan  nan   0.   0.   0.\n",
      "      0.   0.   0.  nan  nan   0.  nan  nan   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan   0.   0.\n",
      "      0.   0.   0.  nan  nan   0.   0.  nan   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan  nan   0.   0.\n",
      "      0.  nan  nan   0.  nan  nan   0.   0.  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.   0.  nan   0.   0.   0.   0.\n",
      "      0.  nan  nan   0.  nan  nan   0.  nan  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan   0.   0.\n",
      "      0.  nan  nan   0.   0.  nan   0.  nan   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan   0.  nan   0.   0.   0.\n",
      "      0.  nan  nan   0.  nan  nan   0.   0.  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan  nan   0.   0.\n",
      "      0.   0.  nan  nan  nan  nan   0.  nan   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.  nan   0.   0.\n",
      "      0.   0.  nan   0.   0.   0.   0.  nan  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan   0.   0.\n",
      "      0.   0.  nan  nan  nan  nan   0.   0.   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan  nan   0.\n",
      "      0.   0.   0.   0.   0.  nan   0.  nan  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.  nan  nan   0.\n",
      "      0.   0.   0.   0.   0.   0.   0.  nan   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan  nan   0.\n",
      "      0.   0.   0.   0.   0.  nan  nan   0.   0.  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan   0.   0.  nan   0.  nan  nan\n",
      "      0.   0.   0.  nan  nan  nan   0.   0.  nan  nan   0.   0.   0.]\n",
      "   [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n",
      "     nan  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.   0.   0.]\n",
      "   [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "   [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "   [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "      0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]]]\n"
     ]
    }
   ],
   "source": [
    "#Relu Backward\n",
    "\n",
    "input_grad=dY_pool\n",
    "Y=Y_relu\n",
    "X=X_relu\n",
    "n_i,c_i,h_i,w_i=Y.shape\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "\n",
    "#input_grad_array=np.ones((n_i,c_i,h_i,w_i),dtype='float64')\n",
    "#output_grad_array=np.array([[[[0.0,0.0,0.0,0.0]]]],dtype='float64')\n",
    "\n",
    "#input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "\n",
    "\n",
    "output_grad=gpuarray.empty((n_i,c_i,h_i,w_i),dtype='float64')\n",
    "#output_grad=gpuarray.to_gpu(output_grad_array)\n",
    "\n",
    "\n",
    "input_grad_data=ctypes.c_void_p(int(input_grad.gpudata))\n",
    "output_grad_data=ctypes.c_void_p(int(output_grad.gpudata))\n",
    "\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "print(\"Setting output Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "\n",
    "print(\"Setting Input Gradient Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "print(\"Setting Output Gradient Descriptor:\"),        \n",
    "#cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type,n_o,c_o,h_o,w_o))\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "#************Activation Backward***********************\n",
    "print(\"Activation Backward Status:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnActivationBackward(handle,activation_desc,ctypes.byref(a),output_desc,\\\n",
    "                Y_data, input_grad_desc, input_grad_data, input_desc, X_data,ctypes.byref(b),\\\n",
    "                 output_grad_desc,output_grad_data))\n",
    "\"\"\"\n",
    "print(\"Activation Input\")\n",
    "print(X.get())\n",
    "print(\"Activation Output\")\n",
    "print(Y.get())\n",
    "print(\"Activation Input Gradient\")\n",
    "print(input_grad.get())\n",
    "\"\"\"\n",
    "print(\"Activation Output Gradient\")\n",
    "print(output_grad.get())\n",
    "\n",
    "dY_relu=output_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,\n",
       "            0.,   0.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  nan,  nan,\n",
       "           nan,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,  nan,\n",
       "            0.,   0.,   0.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,  nan,\n",
       "            0.,   0.,   0.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,  nan,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,\n",
       "            0.,   0.,   0.,   0.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,  nan,  nan,  nan,   0.,  nan,  nan,  nan,   0.,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,  nan,  nan,   0.,  nan,   0.,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,  nan,\n",
       "            0.,   0.,   0.,   0.,  nan,   0.,  nan,  nan,   0.,  nan,  nan,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,  nan,  nan,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,  nan,  nan,   0.,  nan,  nan,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,\n",
       "           nan,   0.,   0.,   0.,   0.,   0.,  nan,  nan,   0.,   0.,  nan,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,  nan,\n",
       "           nan,   0.,   0.,   0.,  nan,  nan,   0.,  nan,  nan,   0.,   0.,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,  nan,   0.,\n",
       "            0.,   0.,   0.,   0.,  nan,  nan,   0.,  nan,  nan,   0.,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,\n",
       "           nan,   0.,   0.,   0.,  nan,  nan,   0.,   0.,  nan,   0.,  nan,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,\n",
       "            0.,   0.,   0.,   0.,  nan,  nan,   0.,  nan,  nan,   0.,   0.,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,  nan,\n",
       "           nan,   0.,   0.,   0.,   0.,  nan,  nan,  nan,  nan,   0.,  nan,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,\n",
       "           nan,   0.,   0.,   0.,   0.,  nan,   0.,   0.,   0.,   0.,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,\n",
       "           nan,   0.,   0.,   0.,   0.,  nan,  nan,  nan,  nan,   0.,   0.,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.,   0.,   0.,   0.,  nan,   0.,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,   0.,\n",
       "           nan,  nan,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  nan,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.,   0.,   0.,   0.,  nan,  nan,   0.,\n",
       "            0.,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,   0.,   0.,  nan,\n",
       "            0.,  nan,  nan,   0.,   0.,   0.,  nan,  nan,  nan,   0.,   0.,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "           nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "           nan,  nan,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=dY_relu.get()\n",
    "a=np.asarray(b)\n",
    "a=a.astype(np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Setting Input Grad Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Output Grad Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Filter Grad Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Input Grad Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Output Grad Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Filter Grad Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Getting Convolution backward Data Algorithm:  CUDNN_STATUS_SUCCESS\n",
      "Get convolution backward Data workspace size:  CUDNN_STATUS_SUCCESS\n",
      "Convolution backward Data:  CUDNN_STATUS_SUCCESS\n",
      "Get convolution backward filter algorithm:  CUDNN_STATUS_SUCCESS\n",
      "Get convolution backward Filter Workspace Size:  CUDNN_STATUS_SUCCESS\n",
      "Convolution Backward Filter:  CUDNN_STATUS_SUCCESS\n",
      "output gradients\n",
      "[[[[              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan  -1.40444776e+306               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan  -1.94906234e+289]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan  -1.94906280e+289               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan  -8.37116099e+298]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan  -8.37116099e+298               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan  -8.37116096e+298  -2.46006311e+260]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]\n",
      "   [              nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan\n",
      "                  nan               nan               nan               nan]]]]\n",
      "filter gradients\n",
      "[[[[ nan  nan]\n",
      "   [ nan  nan]]]]\n"
     ]
    }
   ],
   "source": [
    "#convolution backprop\n",
    "\n",
    "input_grad=dY_relu\n",
    "X=X_conv\n",
    "W=W_conv\n",
    "Y=Y_conv\n",
    "\n",
    "n_i,c_i,h_i,w_i=X_conv.shape\n",
    "n_o,c_o,h_o,w_o=Y_conv.shape\n",
    "k,c_i,h_k,w_k=W_conv.shape\n",
    "\n",
    "#-----------------------initialize-------------------------------------------------------\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "W_data = ctypes.c_void_p(int(W.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(1.0)\n",
    "#input_grad_array=np.ones((n_o,c_o,h_o,w_o),dtype='float64')\n",
    "#input_grad_array=np.random.rand(n_o,c_o,h_o,w_o)\n",
    "#output_grad_array=np.array([[[[0.0,0.0,0.0,0.0]]]],dtype='float64')\n",
    "#input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "input_grad_data=ctypes.c_void_p(int(input_grad.gpudata))\n",
    "\n",
    "print(\"1\")\n",
    "#filter_grad_array=np.ones((n_i,c_i,h_k,w_k),dtype='float64')\n",
    "#filter_grad=gpuarray.to_gpu(filter_grad_array)\n",
    "filter_grad=gpuarray.empty((k,c_i,h_k,w_k),dtype='float64')\n",
    "filter_grad_data=ctypes.c_void_p(int(filter_grad.gpudata))\n",
    "#print(\"2\")\n",
    "#bias_grad_array=np.ones((n_o,c_o,h_o,w_o),dtype='float64')\n",
    "#bias_grad=gpuarray.to_gpu(bias_grad_array)\n",
    "#bias_grad_data=ctypes.c_void_p(int(bias_grad.gpudata))\n",
    "\n",
    "output_grad=gpuarray.empty((n_i,c_i,h_i,w_i),dtype='float64')\n",
    "#output_grad=gpuarray.to_gpu(output_grad_array)\n",
    "output_grad_data=ctypes.c_void_p(int(output_grad.gpudata))\n",
    "\n",
    "\n",
    "#------------------Setting Descriptors--------------------------------------\n",
    "\n",
    "print(\"Setting Input Grad Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_grad_desc,tensor_format,data_type, n_o, c_o, h_o, w_o))\n",
    "print(\"Setting Output Grad Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "print(\"Setting Filter Grad Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetFilter4dDescriptor(filter_grad_desc,data_type,tensor_format,k,c_i,h_k,w_k))\n",
    "#print(\"Setting Bias Grad Descriptor:\"),\n",
    "#cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(bias_grad_desc,tensor_format,data_type, n_o, c_o, h_o, w_o))\n",
    "preference=0\n",
    "memoryLimitInbytes=4*1024\n",
    "data_algo=ctypes.c_int()\n",
    "filter_algo=ctypes.c_int()\n",
    "#algo=0\n",
    "workspace =ctypes.c_void_p()\n",
    "workspace_size=ctypes.c_size_t(0)\n",
    "\n",
    "\n",
    "#------------------Setting Descriptors--------------------------------------\n",
    "\n",
    "print(\"Setting Input Grad Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_grad_desc,tensor_format,data_type, n_o, c_o, h_o, w_o))\n",
    "print(\"Setting Output Grad Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "print(\"Setting Filter Grad Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetFilter4dDescriptor(filter_grad_desc,data_type,tensor_format,k,c_i,h_k,w_k))\n",
    "#print(\"Setting Bias Grad Descriptor:\"),\n",
    "#cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(bias_grad_desc,tensor_format,data_type, n_o, c_o, h_o, w_o))\n",
    "preference=0\n",
    "memoryLimitInbytes=4*1024\n",
    "data_algo=ctypes.c_int()\n",
    "filter_algo=ctypes.c_int()\n",
    "#algo=0\n",
    "workspace =ctypes.c_void_p()\n",
    "workspace_size=ctypes.c_size_t(0)\n",
    "\n",
    "#--------------------Setting functions----------------------------------------\n",
    "print(\"Getting Convolution backward Data Algorithm: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolutionBackwardDataAlgorithm(\\\n",
    "handle,filter_desc,input_grad_desc,conv_desc,\\\n",
    "output_grad_desc, preference, ctypes.c_size_t(memoryLimitInbytes),ctypes.byref(data_algo)))\n",
    "\n",
    "print(\"Get convolution backward Data workspace size: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolutionBackwardDataWorkspaceSize(handle,filter_desc,input_grad_desc,\\\n",
    "                conv_desc,output_grad_desc,data_algo,ctypes.byref(workspace_size)))\n",
    "\n",
    "print(\"Convolution backward Data: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnConvolutionBackwardData(handle,ctypes.byref(a),filter_desc,W_data,input_grad_desc,\\\n",
    "                input_grad_data,conv_desc,data_algo,workspace,ctypes.byref(workspace_size),\\\n",
    "                ctypes.byref(b),output_grad_desc,output_grad_data))\n",
    "\n",
    "print(\"Get convolution backward filter algorithm: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolutionBackwardFilterAlgorithm(handle,input_desc,input_grad_desc,conv_desc,\\\n",
    "        filter_grad_desc,preference,ctypes.c_size_t(memoryLimitInbytes),ctypes.byref(filter_algo)))\n",
    "\n",
    "print(\"Get convolution backward Filter Workspace Size: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolutionBackwardFilterWorkspaceSize(handle,input_desc,input_grad_desc,\\\n",
    "         conv_desc, filter_grad_desc,filter_algo,ctypes.byref(workspace_size)))\n",
    "\n",
    "print(\"Convolution Backward Filter: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnConvolutionBackwardFilter(handle,ctypes.byref(a),input_desc,\\\n",
    "                X_data,input_grad_desc,input_grad_data,conv_desc,filter_algo,workspace,ctypes.byref(workspace_size),\\\n",
    "                 ctypes.byref(b),filter_grad_desc,filter_grad_data))\n",
    "\n",
    "\n",
    "#print \"input to CONVOLUTION\"\n",
    "#print X.get()\n",
    "#print \"input gradients\"\n",
    "#print input_grad.get()\n",
    "print \"output gradients\"\n",
    "print output_grad.get()\n",
    "print \"filter gradients\"\n",
    "print filter_grad.get()\n",
    "#print \"Output\"\n",
    "#print Y.get()\n",
    "\n",
    "#print(\"Convolution Backward Bias: \"),\n",
    "#cudnnCheckStatus(libcudnn.cudnnConvolutionBackwardBias(handle,ctypes.byref(a),input_grad_desc,input_grad_data,bias_grad_desc,bias_grad_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Up\n"
     ]
    }
   ],
   "source": [
    "libcudnn.cudnnDestroyTensorDescriptor(input_desc)\n",
    "libcudnn.cudnnDestroyTensorDescriptor(output_desc)  \n",
    "libcudnn.cudnnDestroyTensorDescriptor(input_grad_desc)\n",
    "libcudnn.cudnnDestroyTensorDescriptor(output_grad_desc) \n",
    "libcudnn.cudnnDestroyConvolutionDescriptor(conv_desc)\n",
    "libcudnn.cudnnDestroyActivationDescriptor(activation_desc)\n",
    "libcudnn.cudnnDestroyPoolingDescriptor(pooling_desc)\n",
    "libcudnn.cudnnDestroyTensorDescriptor(batchnorm_desc)\n",
    "#libcudnn.cudnnDestroyBatchnormDescriptor(batchnorm_grad_desc)\n",
    "skcuda.cublas.cublasDestroy(h)\n",
    "libcudnn.cudnnDestroy(handle)\n",
    "print(\"Cleaned Up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
