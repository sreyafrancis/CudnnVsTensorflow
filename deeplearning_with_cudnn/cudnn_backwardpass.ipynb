{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5110\n",
      "Handle Creation Status: CUDNN_STATUS_SUCCESS\n",
      "Input Tensor Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Input Grad Tensor Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Output Tensor Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Output Grad Tensor Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Filter Descriptor: CUDNN_STATUS_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "#forward pass\n",
    "\n",
    "import sys\n",
    "import ctypes\n",
    "import ctypes.util\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import skcuda.cublas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "try:\n",
    "        libcudnn=ctypes.cdll.LoadLibrary('/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10')\n",
    "except OSError:\n",
    "        print(\"OSError\");\n",
    "print(libcudnn.cudnnGetVersion())\n",
    "libcudnn.cudnnGetErrorString.restype = ctypes.c_char_p\n",
    "libcudnn.cudnnGetErrorString.argtypes = [ctypes.c_int]\n",
    "\n",
    "def cudnnCheckStatus(status):\n",
    "    print(libcudnn.cudnnGetErrorString(status))\n",
    "\n",
    "#**********Defining Handle***************\n",
    "handle=ctypes.c_void_p()\n",
    "print(\"Handle Creation Status:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreate(ctypes.byref(handle)))\n",
    "\n",
    "\n",
    "#************Image Data****************\n",
    "def normalize(image_data):\n",
    "    a = 0.1; b = 0.9; MIN = 0; MAX = 255\n",
    "    b=a + (((image_data - MIN)*(b - a))/(MAX - MIN))\n",
    "    return b\n",
    "\n",
    "img = cv2.imread('One.jpg',0)\n",
    "img=np.asarray(img)\n",
    "img = normalize(img)\n",
    "\n",
    "#************Enums*****************\n",
    "tensor_format=0\n",
    "data_type=1\n",
    "\n",
    "#***********Creating descriptors******\n",
    "input_desc = ctypes.c_void_p()\n",
    "print(\"Input Tensor Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(input_desc)))\n",
    "\n",
    "input_grad_desc = ctypes.c_void_p()\n",
    "print(\"Input Grad Tensor Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(input_grad_desc)))\n",
    "\n",
    "output_desc = ctypes.c_void_p()\n",
    "print(\"Output Tensor Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(output_desc)))\n",
    "\n",
    "output_grad_desc = ctypes.c_void_p()\n",
    "print(\"Output Grad Tensor Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(output_grad_desc)))\n",
    "\n",
    "filter_desc = ctypes.c_void_p()\n",
    "print(\"Filter Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateFilterDescriptor(ctypes.byref(filter_desc)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Filter Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Convolution Desriptor: CUDNN_STATUS_SUCCESS\n",
      "Getting Output Dimensions: CUDNN_STATUS_SUCCESS\n",
      "('Output Dimensions: ', 1, 1, 27, 27)\n",
      "Setting Output Descriptor: CUDNN_STATUS_SUCCESS\n",
      "CUDNN_STATUS_SUCCESS\n",
      "Getting Workspace Size: CUDNN_STATUS_SUCCESS\n",
      "('Workspace Size = ', c_ulong(0L))\n",
      "Workspace Not allocated\n",
      "None\n",
      "Convolution forward Status: CUDNN_STATUS_SUCCESS\n",
      "Convolution Output\n",
      "[[[[ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.89372549  0.9         0.89372549  0.89686275\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.88431373  0.88117647  0.9         0.9         0.9\n",
      "     0.89372549  0.89372549  0.89686275  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88431373  0.88431373  0.9         0.87490196  0.87490196\n",
      "     0.87176471  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.87803922  0.9         0.9         0.77764706  0.4827451   0.41058824\n",
      "     0.57686275  0.86862745  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.86862745  0.64901961  0.23490196  0.12509804\n",
      "     0.33843137  0.74627451  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.88117647  0.88745098  0.89372549  0.70235294  0.24745098  0.1\n",
      "     0.31960784  0.79333333  0.89686275  0.89686275  0.89686275  0.89686275\n",
      "     0.89686275  0.89686275  0.89686275  0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.74313725  0.30392157  0.13137255\n",
      "     0.29764706  0.74941176  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89686275  0.89058824  0.89058824  0.70862745  0.25372549  0.12509804\n",
      "     0.29764706  0.75254902  0.89372549  0.9         0.88431373  0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.85921569  0.6427451   0.23490196  0.1         0.26\n",
      "     0.6145098   0.82470588  0.9         0.89686275  0.9         0.89058824\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89372549  0.9         0.9         0.76509804  0.41058824  0.10313725\n",
      "     0.14392157  0.33215686  0.74        0.89058824  0.9         0.9\n",
      "     0.88117647  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89058824  0.89372549  0.9         0.87803922  0.58941176  0.11882353\n",
      "     0.12196078  0.26941176  0.69921569  0.88431373  0.9         0.9\n",
      "     0.88431373  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.65215686  0.1\n",
      "     0.12509804  0.28823529  0.70235294  0.89058824  0.9         0.9\n",
      "     0.89686275  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89686275  0.9         0.9         0.9         0.65529412  0.1\n",
      "     0.10941176  0.26        0.70862745  0.9         0.89058824  0.9         0.9\n",
      "     0.88745098  0.89686275  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89058824  0.89686275  0.9         0.87176471  0.63333333  0.11568627\n",
      "     0.1345098   0.30392157  0.69607843  0.9         0.89058824  0.9         0.9\n",
      "     0.88745098  0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.66156863  0.11882353\n",
      "     0.1         0.26627451  0.67411765  0.9         0.89372549  0.89686275\n",
      "     0.9         0.88745098  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.87176471  0.9         0.65843137  0.1\n",
      "     0.11882353  0.18156863  0.53607843  0.89058824  0.88117647  0.9         0.9\n",
      "     0.87490196  0.89372549  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.87176471  0.9         0.9         0.9         0.6772549   0.18470588\n",
      "     0.1         0.11568627  0.36666667  0.78392157  0.9         0.9         0.9\n",
      "     0.89372549  0.9         0.88745098  0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.88431373  0.74        0.36666667\n",
      "     0.11882353  0.12196078  0.24745098  0.6427451   0.88745098  0.87803922\n",
      "     0.88117647  0.88745098  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.83411765  0.52352941\n",
      "     0.11568627  0.1         0.22235294  0.5454902   0.9         0.9         0.9\n",
      "     0.89372549  0.89686275  0.87803922  0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88117647  0.9         0.9         0.87490196  0.66470588\n",
      "     0.21607843  0.12823529  0.1         0.40431373  0.83411765  0.87803922\n",
      "     0.9         0.89058824  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89686275  0.89686275  0.88745098  0.89058824  0.78705882\n",
      "     0.35411765  0.10941176  0.1         0.42627451  0.8654902   0.89372549\n",
      "     0.89372549  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.9         0.84039216\n",
      "     0.49215686  0.1627451   0.11568627  0.41372549  0.81215686  0.88745098\n",
      "     0.9         0.9         0.89686275  0.87803922  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.89686275  0.9         0.88745098  0.88431373\n",
      "     0.77137255  0.56117647  0.52666667  0.66470588  0.87490196  0.9         0.9\n",
      "     0.88431373  0.89058824  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]]]]\n"
     ]
    }
   ],
   "source": [
    "#Convolution\n",
    "\n",
    "#*******Enums********************\n",
    "convolution_mode=0\n",
    "algo=0\n",
    "preference=0\n",
    "\n",
    "#**********Dimensions************\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=28\n",
    "w_i=28\n",
    "h_k=2\n",
    "w_k=2\n",
    "k=1\n",
    "pad_h=0\n",
    "pad_w=0\n",
    "stride_h=1\n",
    "stride_w=1\n",
    "upscalex=1\n",
    "upscaley=1\n",
    "\n",
    "#********GPU arrays***********\n",
    "array_type=np.float64\n",
    "#x=np.array([[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0],[13.0,14.0,15.0,16.0],]]],dtype=array_type)\n",
    "#x=np.random.rand(1,1,28,28)\n",
    "x=img.reshape((n_i,c_i,h_i,w_i))\n",
    "X=gpuarray.to_gpu(x)\n",
    "w=np.array([[[1.0,0.0],[0.0,0.0]]],dtype=array_type)\n",
    "W=gpuarray.to_gpu(w)\n",
    "\n",
    "#***********Creating descriptors******\n",
    "conv_desc = ctypes.c_void_p()\n",
    "print(\"Convolution Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateConvolutionDescriptor(ctypes.byref(conv_desc)))\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "print(\"Setting Filter Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetFilter4dDescriptor(filter_desc,data_type,tensor_format,k,c_i,h_k,w_k));\n",
    "print(\"Setting Convolution Desriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetConvolution2dDescriptor(conv_desc,pad_h,pad_w,stride_h,stride_w,upscalex,upscaley,convolution_mode));\n",
    "        \n",
    "\n",
    "#*********configuring the Output*********\n",
    "print(\"Getting Output Dimensions:\"),\n",
    "temp_n_o = ctypes.c_int()\n",
    "temp_c_o = ctypes.c_int()\n",
    "temp_h_o = ctypes.c_int()\n",
    "temp_w_o = ctypes.c_int()\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolution2dForwardOutputDim(conv_desc, input_desc,filter_desc, ctypes.byref(temp_n_o),ctypes.byref(temp_c_o), ctypes.byref(temp_h_o),ctypes.byref(temp_w_o)))\n",
    "\n",
    "n_o=temp_n_o.value\n",
    "c_o=temp_c_o.value\n",
    "w_o=temp_w_o.value\n",
    "h_o=temp_h_o.value\n",
    "print(\"Output Dimensions: \",n_o,c_o,h_o,w_o)\n",
    "Y= gpuarray.empty((n_o,c_o,h_o,w_o), array_type)\n",
    "print(\"Setting Output Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_o,c_o,h_o,w_o));\n",
    "\n",
    "#*********Setting Workspace********\n",
    "algo=ctypes.c_int()\n",
    "memory_limit=ctypes.c_size_t(1024*1024)\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolutionForwardAlgorithm(handle,input_desc,filter_desc,conv_desc,output_desc,preference,memory_limit,ctypes.byref(algo)))\n",
    "\n",
    "workspace =ctypes.c_void_p()\n",
    "workspace_size=ctypes.c_size_t(0)\n",
    "\n",
    "print(\"Getting Workspace Size:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnGetConvolutionForwardWorkspaceSize(handle,input_desc,filter_desc,conv_desc,\\\n",
    "output_desc,algo,ctypes.byref(workspace_size)))\n",
    "print(\"Workspace Size = \",workspace_size)\n",
    "if workspace_size.value!=0:\n",
    "    workspace= drv.mem_alloc(workspace_size.value)\n",
    "    print(\"workspace Allocated\")\n",
    "else:\n",
    "    print(\"Workspace Not allocated\")\n",
    "\n",
    "#*********ConvolutionForward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "W_data = ctypes.c_void_p(int(W.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "print(workspace.value)\n",
    "#workspace_data = ctypes.c_void_p(workspace)\n",
    "print(\"Convolution forward Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnConvolutionForward(handle,ctypes.byref(a),input_desc,X_data,filter_desc,\\\n",
    "                W_data,conv_desc,algo,None,0,ctypes.byref(b),output_desc,Y_data))\n",
    "print(\"Convolution Output\")\n",
    "print(Y.get())\n",
    "\n",
    "Y_conv=Y\n",
    "output_desc_conv=output_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Activation Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Output Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Activation forward Status: CUDNN_STATUS_SUCCESS\n",
      "Relu Activation Output [[[[ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.89372549  0.9         0.89372549  0.89686275\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.88431373  0.88117647  0.9         0.9         0.9\n",
      "     0.89372549  0.89372549  0.89686275  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824  0.89058824  0.89058824  0.89058824\n",
      "     0.89058824  0.89058824  0.89058824]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88431373  0.88431373  0.9         0.87490196  0.87490196\n",
      "     0.87176471  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.87803922  0.9         0.9         0.77764706  0.4827451   0.41058824\n",
      "     0.57686275  0.86862745  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.86862745  0.64901961  0.23490196  0.12509804\n",
      "     0.33843137  0.74627451  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.88117647  0.88745098  0.89372549  0.70235294  0.24745098  0.1\n",
      "     0.31960784  0.79333333  0.89686275  0.89686275  0.89686275  0.89686275\n",
      "     0.89686275  0.89686275  0.89686275  0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.74313725  0.30392157  0.13137255\n",
      "     0.29764706  0.74941176  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89686275  0.89058824  0.89058824  0.70862745  0.25372549  0.12509804\n",
      "     0.29764706  0.75254902  0.89372549  0.9         0.88431373  0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.85921569  0.6427451   0.23490196  0.1         0.26\n",
      "     0.6145098   0.82470588  0.9         0.89686275  0.9         0.89058824\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89372549  0.9         0.9         0.76509804  0.41058824  0.10313725\n",
      "     0.14392157  0.33215686  0.74        0.89058824  0.9         0.9\n",
      "     0.88117647  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89058824  0.89372549  0.9         0.87803922  0.58941176  0.11882353\n",
      "     0.12196078  0.26941176  0.69921569  0.88431373  0.9         0.9\n",
      "     0.88431373  0.9         0.9         0.89372549  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.65215686  0.1\n",
      "     0.12509804  0.28823529  0.70235294  0.89058824  0.9         0.9\n",
      "     0.89686275  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89686275  0.9         0.9         0.9         0.65529412  0.1\n",
      "     0.10941176  0.26        0.70862745  0.9         0.89058824  0.9         0.9\n",
      "     0.88745098  0.89686275  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.89058824  0.89686275  0.9         0.87176471  0.63333333  0.11568627\n",
      "     0.1345098   0.30392157  0.69607843  0.9         0.89058824  0.9         0.9\n",
      "     0.88745098  0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.66156863  0.11882353\n",
      "     0.1         0.26627451  0.67411765  0.9         0.89372549  0.89686275\n",
      "     0.9         0.88745098  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.87176471  0.9         0.65843137  0.1\n",
      "     0.11882353  0.18156863  0.53607843  0.89058824  0.88117647  0.9         0.9\n",
      "     0.87490196  0.89372549  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.87176471  0.9         0.9         0.9         0.6772549   0.18470588\n",
      "     0.1         0.11568627  0.36666667  0.78392157  0.9         0.9         0.9\n",
      "     0.89372549  0.9         0.88745098  0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.88431373  0.74        0.36666667\n",
      "     0.11882353  0.12196078  0.24745098  0.6427451   0.88745098  0.87803922\n",
      "     0.88117647  0.88745098  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.9         0.9         0.83411765  0.52352941\n",
      "     0.11568627  0.1         0.22235294  0.5454902   0.9         0.9         0.9\n",
      "     0.89372549  0.89686275  0.87803922  0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88117647  0.9         0.9         0.87490196  0.66470588\n",
      "     0.21607843  0.12823529  0.1         0.40431373  0.83411765  0.87803922\n",
      "     0.9         0.89058824  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89686275  0.89686275  0.88745098  0.89058824  0.78705882\n",
      "     0.35411765  0.10941176  0.1         0.42627451  0.8654902   0.89372549\n",
      "     0.89372549  0.89372549  0.9         0.89686275  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.89372549  0.9         0.9         0.9         0.84039216\n",
      "     0.49215686  0.1627451   0.11568627  0.41372549  0.81215686  0.88745098\n",
      "     0.9         0.9         0.89686275  0.87803922  0.9         0.9         0.9\n",
      "     0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.88745098  0.89686275  0.9         0.88745098  0.88431373\n",
      "     0.77137255  0.56117647  0.52666667  0.66470588  0.87490196  0.9         0.9\n",
      "     0.88431373  0.89058824  0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9       ]]]]\n",
      "Cleaned Up\n"
     ]
    }
   ],
   "source": [
    "#Relu Activation\n",
    "\n",
    "X=Y_conv\n",
    "#input_desc=output_desc_conv\n",
    "\n",
    "#**********Defining Enumerated Types*************\n",
    "cudnnDataType={'CUDNN_DATA_FLOAT': 0}\n",
    "cudnnTensorFormat = {'CUDNN_TENSOR_NCHW': 0}\n",
    "cudnnNanPropagation     ={'CUDNN_NOT_PROPAGATE_NAN':0}\n",
    "cudnnActivationMode     ={'CUDNN_ACTIVATION_RELU':1}\n",
    "\n",
    "relu_mode = cudnnActivationMode['CUDNN_ACTIVATION_RELU']\n",
    "reluNanOpt = cudnnNanPropagation ['CUDNN_NOT_PROPAGATE_NAN']\n",
    "\n",
    "\n",
    "#**********Dimensions************\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=4\n",
    "w_i=4\n",
    "\"\"\"\n",
    "n_i=n_o\n",
    "c_i=c_o\n",
    "h_i=h_o\n",
    "w_i=w_o\n",
    "\n",
    "\n",
    "#********GPU arrays***********\n",
    "\"\"\"\n",
    "X = gpuarray.to_gpu(np.random.rand(n_i,c_i,h_i,w_i)\n",
    ".astype(np.float32))\n",
    "W = gpuarray.to_gpu(np.random.rand(k,h_k,w_k).astype(np.float32))\n",
    "#x=np.array([[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0],[13.0,14.0,15.0,16.0]]]],dtype='float32')\n",
    "x=np.array([[[[1.0,-2.0,3.0,-4.0],[5.0,-6.0,-7.0,-8.0],[-9.0,-10.0,-11.0,-12.0],[-13.0,14.0,-15.0,-16.0]]]],dtype='float32')\n",
    "X=gpuarray.to_gpu(x)\n",
    "\"\"\"\n",
    "\n",
    "#*******Creating descriptors********\n",
    "\n",
    "activation_desc=ctypes.c_void_p()\n",
    "print(\"Activation Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateActivationDescriptor(ctypes.byref(activation_desc)))\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "reluCeiling=ctypes.c_double(1.0)\n",
    "maxpoolingNanOpt =cudnnNanPropagation['CUDNN_NOT_PROPAGATE_NAN']\n",
    "print(\"Setting Activation Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetActivationDescriptor(activation_desc,relu_mode,reluNanOpt,ctypes.byref(reluCeiling)))\n",
    "\n",
    "\n",
    "#*********configuring the Output*****************************************************************\n",
    "Y = gpuarray.empty((n_i,c_i,h_i,w_i), np.float64)\n",
    "\n",
    "\n",
    "print(\"Setting Output Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "\n",
    "\n",
    "#*********PoolingForward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "print(\"Activation forward Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnActivationForward(handle,activation_desc,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "\n",
    "#*********Display Output*************\n",
    "\n",
    "\n",
    "print(\"Relu Activation Output\"),\n",
    "print(Y.get())\n",
    "\n",
    "\n",
    "libcudnn.cudnnDestroyActivationDescriptor(activation_desc)\n",
    "print(\"Cleaned Up\")\n",
    "\n",
    "Y_relu=Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting Pooling Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Getting Output Dimensions: CUDNN_STATUS_SUCCESS\n",
      "('Output Dimensions: ', 1, 1, 24, 24) Setting Output Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Pooling forward Status: CUDNN_STATUS_SUCCESS\n",
      "Max Pooling Output\n",
      "[[[[ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.77764706  0.86862745  0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.74313725  0.79333333  0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.74313725  0.79333333  0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.76509804  0.75254902  0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.87803922  0.75254902  0.89372549\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.65215686  0.82470588\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.65529412  0.74        0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.65529412  0.70862745\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.66156863  0.70862745\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.66156863  0.70862745\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.6772549   0.69607843\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.74        0.67411765\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.83411765  0.53607843\n",
      "     0.89058824  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.87490196  0.66470588\n",
      "     0.78392157  0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.89058824  0.78705882\n",
      "     0.6427451   0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.84039216\n",
      "     0.5454902   0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.88431373\n",
      "     0.77137255  0.87490196  0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]\n",
      "   [ 0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9         0.9         0.9         0.9         0.9\n",
      "     0.9         0.9         0.9       ]]]]\n",
      "Cleaned Up\n"
     ]
    }
   ],
   "source": [
    "#Maxpool\n",
    "X=Y_relu\n",
    "#**********Defining Enumerated Types*************\n",
    "\n",
    "cudnnDataType={'CUDNN_DATA_FLOAT': 0}\n",
    "cudnnTensorFormat = {'CUDNN_TENSOR_NCHW': 0}\n",
    "cudnnPoolingMode        ={'CUDNN_POOLING_MAX':0}\n",
    "cudnnNanPropagation     ={'CUDNN_NOT_PROPAGATE_NAN':0}\n",
    "\n",
    "pooling_mode=0\n",
    "maxpoolingNanOpt =0\n",
    "\n",
    "#**********Dimensions************\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=4\n",
    "w_i=4\n",
    "\"\"\"\n",
    "n_i=n_o\n",
    "c_i=c_o\n",
    "h_i=h_o\n",
    "w_i=w_o\n",
    "\n",
    "pad_h=0\n",
    "pad_w=0\n",
    "stride_h=1\n",
    "stride_w=1\n",
    "win_h=4\n",
    "win_w=4\n",
    "\n",
    "#********GPU arrays***********\n",
    "#x=np.array([[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0],[13.0,14.0,15.0,16.0]]]],dtype='float32')\n",
    "#X=gpuarray.to_gpu(x)\n",
    "\n",
    "\n",
    "\n",
    "#*******Creating descriptors********\n",
    "\n",
    "pooling_desc=ctypes.c_void_p()\n",
    "print(\"Pooling Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreatePoolingDescriptor(ctypes.byref(pooling_desc)))\n",
    "\n",
    "#********Set descriptors*******\n",
    "\n",
    "print(\"Setting Input Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "print(\"Setting Pooling Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetPooling2dDescriptor(pooling_desc,pooling_mode,maxpoolingNanOpt,\n",
    "win_h,\n",
    "win_w,\n",
    "pad_h,\n",
    "pad_w,\n",
    "stride_h,\n",
    "stride_w))\n",
    "#*********configuring the Output*****************************************************************\n",
    "\n",
    "print(\"Getting Output Dimensions:\"),\n",
    "temp_n_o = ctypes.c_int()\n",
    "temp_c_o = ctypes.c_int()\n",
    "temp_h_o = ctypes.c_int()\n",
    "temp_w_o = ctypes.c_int()\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnGetPooling2dForwardOutputDim(pooling_desc,input_desc,ctypes.byref(temp_n_o),ctypes.byref(temp_c_o), ctypes.byref(temp_h_o),ctypes.byref(temp_w_o)))\n",
    "n_o=temp_n_o.value\n",
    "c_o=temp_c_o.value\n",
    "w_o=temp_w_o.value\n",
    "h_o=temp_h_o.value\n",
    "\n",
    "\n",
    "print(\"Output Dimensions: \",n_o,c_o,h_o,w_o),\n",
    "Y = gpuarray.empty((n_o,c_o,h_o,w_o), np.float64)\n",
    "\n",
    "\n",
    "print(\"Setting Output Descriptor:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_o,c_o,h_o,w_o));\n",
    "\n",
    "\n",
    "#*********PoolingForward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_float(0.0)\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "print(\"Pooling forward Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnPoolingForward(handle,pooling_desc,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "\n",
    "#*********Display Output*************\n",
    "\n",
    "print(\"Max Pooling Output\")\n",
    "print(Y.get())\n",
    "\n",
    "#********Cleaning Up**************\n",
    "\n",
    "libcudnn.cudnnDestroyPoolingDescriptor(pooling_desc)\n",
    "\n",
    "print(\"Cleaned Up\")\n",
    "\n",
    "Y_pool=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batchnorm TensorDescriptor:  CUDNN_STATUS_SUCCESS\n",
      "Setting Input Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output  Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Setting BatchNorm Descriptor:  CUDNN_STATUS_SUCCESS\n",
      "BatchNorm Status: CUDNN_STATUS_SUCCESS\n",
      "Batchnorm Output:\n",
      "[[[[ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.47613948 -0.47122267\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.75267973 -3.02921998\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -0.26381748 -3.02921998\n",
      "     0.08185783  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.24154198\n",
      "    -1.43911354  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.30576023  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -4.68846148\n",
      "    -4.2736511   0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -3.30576023\n",
      "    -4.75759654  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -1.23170835\n",
      "    -7.79953929  0.01272277  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -0.33295254\n",
      "    -4.96500173 -2.33786935  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.01272277\n",
      "    -2.26873429 -5.44894716  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -1.09343823 -7.5921341   0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -0.12554735 -2.6144096  -0.33295254  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]]]]\n"
     ]
    }
   ],
   "source": [
    "# Batchnorm\n",
    "X=Y_pool\n",
    "\n",
    "#**********Defining Enumerated Types*************\n",
    "\"\"\"\n",
    "cudnnDataType={'CUDNN_DATA_FLOAT': 0}\n",
    "cudnnTensorFormat = {'CUDNN_TENSOR_NCHW': 0}\n",
    "cudnnSoftmaxAlgorithm ={'CUDNN_SOFTMAX_FAST':0}\n",
    "\n",
    "BatchNorm Modes\n",
    "CUDNN_BATCHNORM_PER_ACTIVATION : to be used after non-convolution layers. bnBias and bnScale dimensions are 1xCxHxW\n",
    "CUDNN_BATCHNORM_SPATIAL : to be used after convolutional layers. bnBias and bnScale dimensions are 1xCx1x1 \n",
    "\n",
    "cudnnBatchNormMode={'CUDNN_BATCHNORM_PER_ACTIVATION':0,'CUDNN_BATCHNORM_SPATIAL': 1}\n",
    "\"\"\"\n",
    "\n",
    "data_type =1\n",
    "batchnorm_mode=1\n",
    "\n",
    "#**********Dimensions************\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=1\n",
    "w_i=4\n",
    "\"\"\"\n",
    "n_i=n_o\n",
    "c_i=c_o\n",
    "h_i=h_o\n",
    "w_i=w_o\n",
    "\n",
    "#********GPU arrays***********\n",
    "\"\"\"\n",
    "X = gpuarray.to_gpu(np.random.rand(n_i,c_i,h_i,w_i)\n",
    ".astype(np.float32))\n",
    "W = gpuarray.to_gpu(np.random.rand(k,h_k,w_k).astype(np.float32))\n",
    "x=np.array([[[[1.0,2.0,3.0,4.0],[5.0,6.0,7.0,8.0],[9.0,10.0,11.0,12.0],[13.0,14.0,15.0,16.0]]]],dtype='float32')\n",
    "\"\"\"\n",
    "#x=np.array([[[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]]]])\n",
    "#x=np.array([[[[1.0,2.3,4.1,0.6]]]],dtype='float64')\n",
    "#X=gpuarray.to_gpu(x)\n",
    "Y = gpuarray.empty((n_i,c_i,h_i,w_i), np.float64)\n",
    "\n",
    "#*******Creating descriptors********\n",
    "\n",
    "batchnorm_desc=ctypes.c_void_p()\n",
    "print(\"Batchnorm TensorDescriptor: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(batchnorm_desc)))\n",
    "\n",
    "#********Set descriptors*******\n",
    "\n",
    "print(\"Setting Input Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "print(\"Setting Output  Descriptor:\"),        \n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "\n",
    "if batchnorm_mode==0:\n",
    "\tn_bn=1\n",
    "\tc_bn=c_i\n",
    "\th_bn=h_i\n",
    "\tw_bn=w_i\n",
    "else:\n",
    "\tn_bn=1\n",
    "\tc_bn=c_i\n",
    "\th_bn=1\n",
    "\tw_bn=1\n",
    "\n",
    "print(\"Setting BatchNorm Descriptor: \"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(batchnorm_desc,tensor_format,data_type,n_bn,c_bn,h_bn,w_bn))\n",
    "\n",
    "\n",
    "#*********BatchNorm Forward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "scale_array=np.ones((n_bn,c_bn,h_bn,w_bn),dtype='float')\n",
    "scale=gpuarray.to_gpu(scale_array) #gamma\n",
    "scale_p=ctypes.c_void_p(int(scale.gpudata)) \n",
    "\n",
    "bias_array=np.zeros((n_bn,c_bn,h_bn,w_bn),dtype='float32')\n",
    "bias=gpuarray.to_gpu(bias_array) #beta\n",
    "bias_p=ctypes.c_void_p(int(bias.gpudata))\n",
    "\n",
    "exp_avg_factor=ctypes.c_double(0.1) #momentum\n",
    "\n",
    "#running_mean=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float32)\n",
    "#running_var=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float32)\n",
    "\n",
    "running_mean_array=np.zeros((n_bn,c_bn,h_bn,w_bn),dtype='float64')\n",
    "running_mean=gpuarray.to_gpu(running_mean_array)\n",
    "running_mean_p=ctypes.c_void_p(int(running_mean.gpudata))\n",
    "\n",
    "running_var_array=np.ones((n_bn,c_bn,h_bn,w_bn),dtype='float64')\n",
    "running_var=gpuarray.to_gpu(running_var_array)\n",
    "running_var_p=ctypes.c_void_p(int(running_var.gpudata))\n",
    "\n",
    "#e=ctypes.c_float(libcudnn.CUDNN_BN_MIN_EPSILON)\n",
    "epsilon=ctypes.c_double(0.0001)\n",
    "\n",
    "result_save_mean=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float64)\n",
    "result_save_mean_p=ctypes.c_void_p(int(result_save_mean.gpudata))\n",
    "\n",
    "result_save_var=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float64)\n",
    "result_save_var_p=ctypes.c_void_p(int(result_save_var.gpudata))\n",
    "\n",
    "print(\"BatchNorm Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnBatchNormalizationForwardTraining(handle,batchnorm_mode,ctypes.byref(a),\\\n",
    "                 ctypes.byref(b),input_desc,X_data,output_desc,Y_data,batchnorm_desc,scale_p,bias_p,\\\n",
    "                 exp_avg_factor,running_mean_p,running_var_p,epsilon,result_save_mean_p,result_save_var_p))\n",
    "\n",
    "print(\"Batchnorm Output:\")\n",
    "print(Y.get())\n",
    "Y_norm=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_STATUS_SUCCESS\n",
      "CUDNN_STATUS_SUCCESS\n",
      "CUDNN_STATUS_SUCCESS\n",
      "CUDNN_STATUS_SUCCESS\n",
      "Adding Bias Tensor Status:\n",
      "CUDNN_STATUS_SUCCESS\n",
      "Input vector\n",
      "----------------------------------\n",
      "[[[[ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.47613948 -0.47122267\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.75267973 -3.02921998\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -0.26381748 -3.02921998\n",
      "     0.08185783  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.24154198\n",
      "    -1.43911354  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.30576023  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -4.68846148\n",
      "    -4.2736511   0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -3.30576023\n",
      "    -4.75759654  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -1.23170835\n",
      "    -7.79953929  0.01272277  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -0.33295254\n",
      "    -4.96500173 -2.33786935  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.01272277\n",
      "    -2.26873429 -5.44894716  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -1.09343823 -7.5921341   0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -0.12554735 -2.6144096  -0.33295254  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]]]]\n",
      "Weight Vector\n",
      "----------------------------------\n",
      "[[ 0.16839253  0.13594244  0.75246996 ...,  0.13983986  0.27517943\n",
      "   0.48091562]\n",
      " [ 0.57096578  0.47972662  0.95884972 ...,  0.66231687  0.46827457\n",
      "   0.04648281]\n",
      " [ 0.87491523  0.5030556   0.05238295 ...,  0.45924295  0.74838038\n",
      "   0.90969492]\n",
      " ..., \n",
      " [ 0.39909871  0.67891075  0.61869977 ...,  0.73945749  0.09674232\n",
      "   0.5497003 ]\n",
      " [ 0.95717906  0.48556157  0.49453132 ...,  0.92043811  0.96222531\n",
      "   0.34169807]\n",
      " [ 0.68366911  0.53398526  0.28560169 ...,  0.10670616  0.88984177\n",
      "   0.88293877]]\n",
      "Inner product\n",
      "----------------------------------\n",
      "[[ 0.57407927 -0.12504272  7.68217724  0.20842994  6.67241621  2.90177525\n",
      "   4.68101873  4.68644653 -2.06564615  7.32289596]]\n"
     ]
    }
   ],
   "source": [
    "#Innerproduct\n",
    "A=Y_norm\n",
    "#skcuda.cublas.cublasCheckStatus(status)\n",
    "\"\"\"\n",
    "m=1\n",
    "k=3\n",
    "n=4\n",
    "\"\"\"\n",
    "m=1\n",
    "k=n_o*c_o*h_o*w_o\n",
    "#k=100\n",
    "n=10\n",
    "#x = np.float64(np.random.rand(m,k))\n",
    "w = np.float64(np.random.rand(k,n))\n",
    "bias=np.zeros((m,n),dtype=\"float64\")\n",
    "\n",
    "\n",
    "#A = gpuarray.to_gpu(x)\n",
    "B = gpuarray.to_gpu(w)\n",
    "C =gpuarray.empty((m,n), np.float64)\n",
    "BIAS =gpuarray.to_gpu(bias)\n",
    "h =skcuda.cublas.cublasCreate()\n",
    "\n",
    "#transa=_CUBLAS_OP[transa]\n",
    "\n",
    "lda=m\n",
    "ldb=k\n",
    "ldc=m\n",
    "#alf = 1.0;\n",
    "#bet = 0.0;\n",
    "#const float *alpha = &alf;\n",
    "#const float *beta = &bet;\n",
    "alpha=1.0\n",
    "beta=0.0\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(input_desc)))\n",
    "cudnnCheckStatus(libcudnn.cudnnCreateTensorDescriptor(ctypes.byref(output_desc)))\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "\n",
    "\n",
    "#d=skcuda.cublas.cublasSgemm(h, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc)\n",
    "status=skcuda.cublas.cublasDgemm(h, 0, 0, m, n, k, alpha,A.gpudata, lda,B.gpudata, ldb, beta,C.gpudata, ldc)\n",
    "#skcuda.cublas.cublasCheckStatus(status)\n",
    "skcuda.cublas.cublasDestroy(h)\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(1.0)\n",
    "X_data = ctypes.c_void_p(int(BIAS.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(C.gpudata))\n",
    "\n",
    "\n",
    "print(\"Adding Bias Tensor Status:\")\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnAddTensor(handle,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "print 'Input vector'\n",
    "print '----------------------------------'\n",
    "print A.get()\n",
    "print 'Weight Vector'\n",
    "print '----------------------------------'\n",
    "print w\n",
    "print 'Inner product'\n",
    "print '----------------------------------'\n",
    "print C.get()\n",
    "\n",
    "n_o=1\n",
    "c_o=1\n",
    "h_o=m\n",
    "w_o=n\n",
    "\n",
    "\n",
    "Y_ip=C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Input Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output  Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Softmax Activation Status: CUDNN_STATUS_SUCCESS\n",
      "Softmax Output\n",
      "[[[[  3.76739342e-04   1.87247554e-04   4.60308188e-01   2.61361425e-04\n",
      "      1.67693048e-01   3.86319381e-03   2.28908606e-02   2.30154454e-02\n",
      "      2.68919808e-05   3.21377024e-01]]]]\n",
      "Cleaned Up\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "X=Y_ip\n",
    "\n",
    "\n",
    "#*******Enums************\n",
    "#softmax_mode = cudnnSoftmaxMode['CUDNN_SOFTMAX_MODE_INSTANCE']\n",
    "#softmax_algo = cudnnSoftmaxAlgorithm ['CUDNN_SOFTMAX_FAST']\n",
    "softmax_mode=0\n",
    "softmax_algo=0\n",
    "#**********Dimensions************\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=1\n",
    "w_i=4\n",
    "\"\"\"\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=1\n",
    "w_i=n_o*c_o*h_o*w_o\n",
    "\n",
    "#********GPU arrays***********\n",
    "#x=np.array([[[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]]]])\n",
    "#x=np.array([[[[1.0,2.3,4.1,0.6]]]],dtype='float32')\n",
    "#X=gpuarray.to_gpu(x)\n",
    "\n",
    "Y = gpuarray.empty((n_i,c_i,h_i,w_i), np.float64)\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "print(\"Setting Output  Descriptor:\"),        \n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_desc,tensor_format,data_type,n_i,c_i,h_i,w_i))\n",
    "\n",
    "#*********Softmax Forward********\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(0.0)\n",
    "X_data = ctypes.c_void_p(int(X.gpudata))\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "\n",
    "print(\"Softmax Activation Status:\"),\n",
    "\n",
    "cudnnCheckStatus(libcudnn.cudnnSoftmaxForward(handle,softmax_algo,softmax_mode,ctypes.byref(a),input_desc,X_data,ctypes.byref(b),output_desc,Y_data))\n",
    "\n",
    "print(\"Softmax Output\")\n",
    "print(Y.get())\n",
    "Y_softmax=Y\n",
    "\n",
    "\n",
    "libcudnn.cudnnDestroyTensorDescriptor(input_desc)\n",
    "libcudnn.cudnnDestroyTensorDescriptor(output_desc)  \n",
    "\n",
    "libcudnn.cudnnDestroy(handle)\n",
    "print(\"Cleaned Up\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7880e7b0f831>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mbb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_grad' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bb=np.array(output_grad.get())\n",
    "print bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Input Gradient Descriptor CUDNN_STATUS_SUCCESS\n",
      "Setting Output Gradient Descriptor: CUDNN_STATUS_SUCCESS\n",
      "Softmax Backward Status: CUDNN_STATUS_BAD_PARAM\n",
      "Softmax Output\n",
      "[[[[  3.76739342e-04   1.87247554e-04   4.60308188e-01   2.61361425e-04\n",
      "      1.67693048e-01   3.86319381e-03   2.28908606e-02   2.30154454e-02\n",
      "      2.68919808e-05   3.21377024e-01]]]]\n",
      "Softmax Input Gradient\n",
      "[[[[ 1.   1.   1.   1.1  1.1  1.   1.   1.   1.   1. ]]]]\n",
      "Softmax Output Gradient\n",
      "[[[[ 1.   1.   1.   1.1  1.1  1.   1.   1.   1.   1. ]]]]\n"
     ]
    }
   ],
   "source": [
    "#Softmax backward\n",
    "\n",
    "n_o=1\n",
    "c_o=1\n",
    "h_o=1\n",
    "w_o=10\n",
    "\n",
    "n_i=1\n",
    "c_i=1\n",
    "h_i=1\n",
    "w_i=10\n",
    "\n",
    "Y= Y_softmax\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "#print(Y.get())\n",
    "a=ctypes.c_double(1.0)\n",
    "b=ctypes.c_double(1.0)\n",
    "input_grad_array=np.array([[[[1.0,1.0,1.0,1.1,1.1,1.0,1.0,1.0,1.0,1.0]]]],dtype='float64')\n",
    "output_grad_array=np.zeros((n_o,c_o,h_o,w_o),dtype='float64')\n",
    "\n",
    "input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "#print(input_grad.get())\n",
    "\n",
    "output_grad=gpuarray.empty((n_o,c_o,h_o,w_o),np.float64)\n",
    "#output_grad=gpuarray.to_gpu(output_grad_array)\n",
    "#print(output_grad.get())\n",
    "\n",
    "\n",
    "input_grad_data=ctypes.c_void_p(int(input_grad.gpudata))\n",
    "output_grad_data=ctypes.c_void_p(int(output_grad.gpudata))\n",
    "\n",
    "\n",
    "#********Set descriptors*******\n",
    "print(\"Setting Input Gradient Descriptor\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(input_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "print(\"Setting Output Gradient Descriptor:\"),        \n",
    "#cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type,n_o,c_o,h_o,w_o))\n",
    "cudnnCheckStatus(libcudnn.cudnnSetTensor4dDescriptor(output_grad_desc,tensor_format,data_type, n_i, c_i, h_i, w_i))\n",
    "\n",
    "#************Softmax Backward***********************\n",
    "print(\"Softmax Backward Status:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnSoftmaxBackward(handle, softmax_algo, softmax_mode, ctypes.byref(a),input_desc,\\\n",
    "                 Y_data, input_grad_desc, input_grad_data, ctypes.byref(b),output_grad_desc, output_grad_data))\n",
    "\n",
    "print(\"Softmax Output\")\n",
    "print(Y.get())\n",
    "print(\"Softmax Input Gradient\")\n",
    "print(input_grad.get())\n",
    "print(\"Softmax Output Gradient\")\n",
    "print(output_grad.get())\n",
    "dX_softmax=output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = np.array(output_grad.get())\n",
    "print ss.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "[[[[ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.47613948 -0.47122267\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -3.23662516 -2.13046416\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -2.75267973 -3.02921998\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796 -0.26381748 -3.02921998\n",
      "     0.08185783  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.24154198\n",
      "    -1.43911354  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.30576023  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.17240691\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -5.03413679\n",
      "    -3.99711085  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -4.68846148\n",
      "    -4.2736511   0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -3.30576023\n",
      "    -4.75759654  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -1.23170835\n",
      "    -7.79953929  0.01272277  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796 -0.33295254\n",
      "    -4.96500173 -2.33786935  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.01272277\n",
      "    -2.26873429 -5.44894716  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -1.09343823 -7.5921341   0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "    -0.12554735 -2.6144096  -0.33295254  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]\n",
      "   [ 0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796\n",
      "     0.22012796  0.22012796  0.22012796  0.22012796  0.22012796  0.22012796]]]]\n",
      "X\n",
      "[[ 0.16839253  0.13594244  0.75246996 ...,  0.13983986  0.27517943\n",
      "   0.48091562]\n",
      " [ 0.57096578  0.47972662  0.95884972 ...,  0.66231687  0.46827457\n",
      "   0.04648281]\n",
      " [ 0.87491523  0.5030556   0.05238295 ...,  0.45924295  0.74838038\n",
      "   0.90969492]\n",
      " ..., \n",
      " [ 0.39909871  0.67891075  0.61869977 ...,  0.73945749  0.09674232\n",
      "   0.5497003 ]\n",
      " [ 0.95717906  0.48556157  0.49453132 ...,  0.92043811  0.96222531\n",
      "   0.34169807]\n",
      " [ 0.68366911  0.53398526  0.28560169 ...,  0.10670616  0.88984177\n",
      "   0.88293877]]\n",
      "IP output\n",
      "[[ 0.57407927 -0.12504272  7.68217724  0.20842994  6.67241621  2.90177525\n",
      "   4.68101873  4.68644653 -2.06564615  7.32289596]]\n",
      "input gradient\n",
      "[[[[ 1.   1.   1.   1.1  1.1  1.   1.   1.   1.   1. ]]]]\n",
      "output gradient (dx)\n",
      "[[ 2.24530518]\n",
      " [ 2.24530518]\n",
      " [ 2.24530518]]\n",
      "Weights gradient (dw)\n",
      "[[ 0.16839253  0.16839253  0.16839253  0.18523179  0.18523179  0.16839253\n",
      "   0.16839253  0.16839253  0.16839253  0.16839253]\n",
      " [ 0.13594244  0.13594244  0.13594244  0.14953669  0.14953669  0.13594244\n",
      "   0.13594244  0.13594244  0.13594244  0.13594244]\n",
      " [ 0.75246996  0.75246996  0.75246996  0.82771695  0.82771695  0.75246996\n",
      "   0.75246996  0.75246996  0.75246996  0.75246996]]\n"
     ]
    }
   ],
   "source": [
    "#inner Product Backward\n",
    "m=3\n",
    "k=10\n",
    "n=1 \n",
    "\n",
    "\n",
    "\n",
    "#input_grad_array=np.array([[1],[2]],dtype='float64')\n",
    "#input_grad_array=np.reshape(input_grad_array,(m,k))\n",
    "#input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "input_grad=dX_softmax\n",
    "\n",
    "output_grad=gpuarray.empty((m,n),np.float64)\n",
    "\n",
    "#ldc=n\n",
    "h =skcuda.cublas.cublasCreate()\n",
    "alpha=1.0\n",
    "beta=0.0\n",
    "lda=m\n",
    "ldb=k\n",
    "ldc=m\n",
    "\n",
    "status=skcuda.cublas.cublasDgemm(h, 0, 0, m, n, k, alpha,A.gpudata, lda,input_grad.gpudata,ldb, beta,\\\n",
    "    output_grad.gpudata, ldc)\n",
    "\n",
    "skcuda.cublas.cublasDestroy(h)\n",
    "#B=B.reshape((1,-1))\n",
    "#input_grad=input_grad.reshape((-1,1))\n",
    "\n",
    "h =skcuda.cublas.cublasCreate()\n",
    "m=10\n",
    "k=1 #2\n",
    "n=3 #3\n",
    "lda=m\n",
    "ldb=k\n",
    "ldc=m\n",
    "weight_grad=gpuarray.empty((n,m),np.float64)\n",
    "status=skcuda.cublas.cublasDgemm(h, 0, 0, m, n, k, alpha,input_grad.gpudata, lda,B.gpudata,ldb, beta,\\\n",
    "    weight_grad.gpudata, ldc)\n",
    "skcuda.cublas.cublasDestroy(h)\n",
    "\n",
    "print(\"W\")\n",
    "print(A.get())\n",
    "print(\"X\")\n",
    "print(B.get())\n",
    "\n",
    "print(\"IP output\")\n",
    "print(C.get())\n",
    "print(\"input gradient\")\n",
    "print(input_grad.get())\n",
    "print(\"output gradient (dx)\")\n",
    "print(output_grad.get())\n",
    "\n",
    "print(\"Weights gradient (dw)\")\n",
    "print(weight_grad.get())\n",
    "\n",
    "dX_innerproduct=output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Norm Backward status: CUDNN_STATUS_BAD_PARAM\n",
      "Batchnorm Output\n",
      "[[ 0.57407927 -0.12504272  7.68217724  0.20842994  6.67241621  2.90177525\n",
      "   4.68101873  4.68644653 -2.06564615  7.32289596]]\n",
      "Batchnorm Input Gradient\n",
      "[[ 2.24530518]\n",
      " [ 2.24530518]\n",
      " [ 2.24530518]]\n",
      "Batchnorm Output Gradient\n",
      "[[[[  1.14016224e-187   2.74735249e+123   2.66262241e+123   1.11778711e+138\n",
      "      1.50326261e-269  -1.37695515e+228   1.32437141e+123   1.40958566e+123\n",
      "      2.40514545e-268   1.32340306e+123]]]]\n"
     ]
    }
   ],
   "source": [
    "#Batchnorm Backward\n",
    "X=\n",
    "Y=\n",
    "Y_data = ctypes.c_void_p(int(Y.gpudata))\n",
    "batchnorm_grad_desc = batchnorm_desc\n",
    "input_grad_desc=input_desc\n",
    "output_grad_desc=output_desc\n",
    "\n",
    "#input_grad_array=np.array([[[[1,1,1,1]]]],dtype='float64')\n",
    "#input_grad=gpuarray.to_gpu(input_grad_array)\n",
    "input_grad=dX_innerproduct\n",
    "input_grad_data=ctypes.c_void_p(int(input_grad.gpudata))\n",
    "\n",
    "output_grad=gpuarray.empty((n_i,c_i,h_i,w_i),np.float64)\n",
    "output_grad_data=ctypes.c_void_p(int(output_grad.gpudata))\n",
    "\n",
    "scale_grad=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float64)\n",
    "scale_grad_p=ctypes.c_void_p(int(scale_grad.gpudata))\n",
    "bias_grad=gpuarray.empty((n_bn,c_bn,h_bn,w_bn),np.float64)\n",
    "bias_grad_p=ctypes.c_void_p(int(bias_grad.gpudata))\n",
    "\n",
    "a_data=ctypes.c_double(1.0)\n",
    "b_data=ctypes.c_double(0.0)\n",
    "a_param=ctypes.c_double(1.0)\n",
    "b_param=ctypes.c_double(0.0)\n",
    "\n",
    "print(\"Batch Norm Backward status:\"),\n",
    "cudnnCheckStatus(libcudnn.cudnnBatchNormalizationBackward(handle,batchnorm_mode,\\\n",
    "        ctypes.byref(a_data),ctypes.byref(b_data),ctypes.byref(a_param),ctypes.byref(b_param),\\\n",
    "            output_desc,Y_data,input_grad_desc,input_grad_data,output_grad_desc,output_grad_data,\\\n",
    "            batchnorm_grad_desc,scale_p,scale_grad_p,bias_grad_p,epsilon,result_save_mean_p,result_save_var_p)) \n",
    "\n",
    "print(\"Batchnorm Output\")\n",
    "print(Y.get())\n",
    "print(\"Batchnorm Input Gradient\")\n",
    "print(input_grad.get())\n",
    "print(\"Batchnorm Output Gradient\")\n",
    "print(output_grad.get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "libcudnn.cudnnDestroyTensorDescriptor(input_desc)\n",
    "libcudnn.cudnnDestroyTensorDescriptor(output_desc)  \n",
    "libcudnn.cudnnDestroyTensorDescriptor(input_grad_desc)\n",
    "libcudnn.cudnnDestroyTensorDescriptor(output_grad_desc) \n",
    "skcuda.cublas.cublasDestroy(h)\n",
    "libcudnn.cudnnDestroy(handle)\n",
    "print(\"Cleaned Up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
